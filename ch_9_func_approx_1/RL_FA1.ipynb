{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Value Function Approximation</center>\n",
    "## <center>Part I</center>\n",
    "## <center>Reference: Sutton and Barto;Chapter 9-11</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Table of Contents</center>\n",
    "<br>\n",
    "\n",
    "* **Introduction**<br><br>\n",
    "\n",
    "* **Iterative Policy Evaluation Methods**<br><br>\n",
    "\n",
    "* **Iterative Control Methods**<br><br>\n",
    "\n",
    "* **Average Reward Setting**<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Introduction</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Reinforcement learning can be used to solve large problems, e.g.\n",
    "<br><br>\n",
    "* Backgammon: $10^{20}$ states\n",
    "<br><br>\n",
    "* Computer Go: $10^{170}$ states\n",
    "<br><br>\n",
    "* Helicopter: continuous state space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How can we use the methods learnt previously on such huge state-spaces?\n",
    "<br><br><br>\n",
    "For eg: A Tabular methods using only Value Function of states (In half Precision) for the game of Backgammon:\n",
    "\n",
    "* Memory Requirement: $\\frac{10^{20} \\times 2}{10^{15}} = $2 Lac PetaBytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Story so far...\n",
    "\n",
    "* So far we have represented value function by a lookup table;\n",
    "<br><br>\n",
    "* Every state $s$ has an entry $V(s)$<br>\n",
    "  Or every state-action pair $s,a$ has an entry $Q(s, a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Problem with large MDPs:\n",
    "  * There are too many states and/or actions to store in memory\n",
    "  * It is too slow to learn the value of each state individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Solution for large MDPs**:\n",
    "  * Estimate value function with function approximation<br>\n",
    "### $$\\hat{v}(s, w) \\sim v_\\pi(s)$$ <br>\n",
    "    or<br>\n",
    "### $$\\hat{q}(s, a, w) \\sim q_\\pi(s, a)$$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Generalise from seen states to unseen states;<br>\n",
    "\n",
    "* Update parameter w using MC or TD learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Value Approximation\n",
    "### Types of Value Function Approximation\n",
    "\n",
    "<center><img src=\"img/func_approx.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Value Approximation\n",
    "### Function Approximation methods:\n",
    "<br>\n",
    "* There are many function approximators, e.g.<br><br>\n",
    "  * Linear combinations of features<br><br>\n",
    "  * Neural network<br><br>\n",
    "  * Decision tree<br><br>\n",
    "  * Nearest neighbour<br><br>\n",
    "  * Fourier / wavelet bases<br><br>\n",
    "  * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Value Approximation\n",
    "### Some Major Issues\n",
    "\n",
    "* Not all function approximation methods are equally well suited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Many statistical methods assume a **static training set** over which multiple passes are made.\n",
    "  * Our situation:data is aquired Online.\n",
    "  * Essential to learn efficiently from incrementally acquired data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Reinforcement learning generally requires approximation methods able to handle **nonstationary target** functions\n",
    "  * In General Control Mechanism, we often seek to learn $q_\\pi$ while $\\pi$ changes. \n",
    "  * Bootstrapping methods: The target values of training examples are nonstationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* **Methods that cannot easily handle such nonstationarity are less suitable for reinforcement learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Value Approximation\n",
    "### Which Function Approximator\n",
    "<br>\n",
    "* We consider **differentiable** function approximators, e.g.<br><br>\n",
    "  * **Linear combinations of features**<br><br>\n",
    "  * **Neural network**<br><br>\n",
    "  * Decision tree<br><br>\n",
    "  * Nearest neighbour<br><br>\n",
    "  * Fourier / wavelet bases<br><br>\n",
    "  * ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Incremental Methods</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "<center><img src=\"img/fa_slide1.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  The Prediction Objective (MSVE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Mean Squared Value Error, or MSVE:\n",
    "### $$ MSVE(\\theta) = \\sum_{s \\in S}^{}{d(s) \\big[ v_\\pi (s)-\\hat(v)(s,\\theta) \\big]^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Weighting or distribution $d(s) \\ge 0$ representing how much we care about the error in each state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Why was $d(s)$ not required in Tabular methods?\n",
    "  * because the learned value function could come to equal the true value function exactly.\n",
    "  * learned values at each state were decoupled—an update at one state affected no other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **More states than weights**\n",
    "  * making one state’s estimate more accurate leads to making others’ less accurate. \n",
    "  * We must specify which states we care most about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  The Prediction Objective (MSVE)\n",
    "### Is MSVE Perfect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Our ultimate purpose is to use it in finding a better policy.\n",
    "  * **The best value function for this purpose is not necessarily the best for minimizing MSVE. **\n",
    "  \n",
    "* Generally,if not specified, consider $d(s) = 1, \\forall s \\in S $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Stochastic Gradient Descent for MSVE</center>\n",
    "<center><img src=\"img/fa_slides2.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Feature Vector</center>\n",
    "\n",
    "<center><img src=\"img/fa_slides3.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Linear Value Function Approximation</center>\n",
    "\n",
    "<center><img src=\"img/fa_slides4.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Table Look-up</center>\n",
    "\n",
    "<center><img src=\"img/fa_slides5.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Incremental Prediction Methods</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Incremental Prediction Methods</center>\n",
    "\n",
    "<center><img src=\"img/fa_slides6.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Important Point.\n",
    "\n",
    "* The **Monte Carlo target** $U_t = G_t$ is by definition an unbiased estimate of $v_\\pi(S_t)$.\n",
    "  * In this situation, SGD method converges to a locally optimal approximation to $v_\\pi(S_t)$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Bootstrapping targets** such as $n$-step returns $G^{(n)}_{t}$ or the DP target, all depend on the current value of the weight vector $\\theta_t$.\n",
    "  * Implies that they will be biased\n",
    "  * will not produce a true gradient-descent method\n",
    "  * we call them semi-gradient methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Disadvantage**:semi-gradient (bootstrapping) methods do not converge as robustly as gradient methods.\n",
    "  * they do converge reliably in cases such as the linear approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Advantage**:\n",
    "  * Typically significantly faster to learn\n",
    "  * Enable learning to be continual and online\n",
    "  * provides computational advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Monte-Carlo with Value Function Approximation</center>\n",
    "\n",
    "<center><img src=\"img/fa_slides7.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>TD Learning with Value Function Approximation</center>\n",
    "\n",
    "<center><img src=\"img/fa_slides8.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>TD($\\lambda$) with Value Function Approximation</center>\n",
    "\n",
    "<center><img src=\"img/fa_slides9.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>TD($\\lambda$) with Value Function Approximation</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* TD converges to a point called the **TD-fixedpoint**(see references)\n",
    "\n",
    "### $$MSVE(\\theta_{TD}) \\le \\frac{1}{1-\\gamma} min_\\theta(MSVE(\\theta))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\gamma$ is often near one\n",
    "  * the expansion factor can be quite large, \n",
    "  * there can be substantial loss in asymptotic performance with the TD method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* However, recall that the TD methods often\n",
    "  * have vastly reduced variance compared to Monte Carlo methods, and \n",
    "  * are faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>State Aggregation on the 1000-state Random Walk </center>\n",
    "\n",
    "*  A simple form of generalizing function approximation in which states are grouped together.\n",
    "\n",
    "* One estimated value (one component of the weight vector $\\theta$) for each group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A 1000-state version of the random walk task\n",
    "* States are numbered from 1 to 1000, left to right,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* all episodes begin near the center, in state 500\n",
    "* State transitions are from the current state to one of the 100 neighboring states to its left or right\n",
    "  * With equal probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Termination on the left produces a reward of −1, and termination on the right produces a reward of +1.\n",
    "* All other transitions have a reward of zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A 1000-state Random Walk\n",
    "### With Monte Carlo Estimates\n",
    "<center><img src=\"img/fa_prob1.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>\n",
    "\n",
    "### Importance of the distribution function d(s)\n",
    "\n",
    "* Do you notice the skew in the prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A 1000-state Random Walk\n",
    "### With TD estimates\n",
    "<center><img src=\"img/fa_prob2.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 800px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  <center>Iterative Control Approximation</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Control with Value Function Approximation</center>\n",
    "\n",
    "<center><img src=\"img/fa_slides10.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Action-Value Function Approximation</center>\n",
    "\n",
    "<center><img src=\"img/fa_slides11.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Linear Action-Value Function Approximation</center>\n",
    "\n",
    "<center><img src=\"img/fa_slides12.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Incremental Control Algorithms</center>\n",
    "\n",
    "<center><img src=\"img/fa_slides13.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mountain Car Problem\n",
    "### Live Demo\n",
    "\n",
    "<center><img src=\"img/fa_prob3.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mountain Car Problem\n",
    "\n",
    "<center><img src=\"img/fa_prob4.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Single Step vs Multi Step\n",
    "\n",
    "* **Episodic semi-gradient one-step Sarsa**\n",
    "  * Target: \n",
    "### $$ G_t = R_{t+1}+ \\gamma  \\hat{q}(S_{t+1},A_{t+1},\\theta _t)$$\n",
    "  * Gradient:\n",
    "### $$ \\theta_{t+1} = \\theta_t + \\alpha[ G_t -\\hat{q}(S_t,A_t,\\theta_t)] \\nabla\\hat{q}(S_t,A_t,\\theta_t) $$\n",
    "\n",
    "* ** Episodic n-step Semi-gradient Sarsa**\n",
    "  * Target: \n",
    "### $$ G_t^{(n)} = R_{t+1}+\\gamma R_{t+2} + ... + \\gamma^{n-1}R(t+n) +\\gamma^n  \\hat{q}(S_{t+n},A_{t+n},\\theta _{t+n-1})$$\n",
    "  * Gradient:\n",
    "### $$ \\theta_{t+n} = \\theta_{t+n-1} + \\alpha[ G_t^{(n)} -\\hat{q}(S_t,A_t,\\theta_{t+n-1})] \\nabla\\hat{q}(S_t,A_t,\\theta_{t+n-1}) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Single Step vs Multi Step\n",
    "\n",
    "<center><img src=\"img/fa_prob5.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Effect of learning rate\n",
    "\n",
    "<center><img src=\"img/fa_prob6.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Study of λ: Should We Bootstrap?</center>\n",
    "\n",
    "<center><img src=\"img/fa_slides14.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 800px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Average Reward</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Average Reward\n",
    "\n",
    "* Classical Settings:\n",
    "  * Discounting Setting\n",
    "  * Episodic Setting\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Average Reward**\n",
    "  * applies to continuing problems\n",
    "  * there is no discounting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* **Why?**\n",
    "  * The discounted setting is problematic with function approximation, \n",
    "  * The average-reward setting is needed to replace it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Average Reward\n",
    "\n",
    "* In average reward setting, the quality of a policy $\\pi$ is defined as:\n",
    "  * The **average rate** of rewards while following that policy, $\\eta (\\pi)$<br>\n",
    "  ### $$ \\eta(\\pi) = lim_{T \\to \\infty}\\frac{1}{T} \\sum_{t=1}^{T}{E[R_t|A_{0:t-1} \\sim \\pi]}$$\n",
    "  ### $$ \\eta(\\pi) = \\sum_{s}{d_\\pi(s) \\sum_{a}{\\pi(a|s) \\sum_{s',r}{p(s,r'|s,a)r}}}$$\n",
    "  \n",
    "  * $d_\\pi$ is the steady-state distribution.\n",
    "  ### $$ \\sum_{s}{d_\\pi(s) \\sum_{a}{\\pi(a|s,\\theta) {p(s'|s,a)}}} = d_\\pi(s')$$\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Average Reward\n",
    "\n",
    "* In the average-reward setting, returns are defined in terms of differences between rewards and the average reward:\n",
    "  ### $$G_t = R_{t+1} - \\eta(\\pi) + R_{t+2} - \\eta(\\pi) + ...$$\n",
    "\n",
    "* This is know as the **differential return**\n",
    "  * corresponding value functions are known as **differential value functions**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Contrast with Discounted Value functions:\n",
    "\n",
    "* Differential value functions also have Bellman equations very similar to disconted value functions.\n",
    "\n",
    "* For disconted value functions\n",
    "<center><img src=\"img/fa_avg5.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 400px;\"/></center>\n",
    "<center><img src=\"img/fa_avg6.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 400px;\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* For Average reward value Functions\n",
    "<center><img src=\"img/fa_avg1.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 400px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Contrast with Discounted Value functions:\n",
    "\n",
    "* For optimal disconted value functions\n",
    "<center><img src=\"img/fa_avg3.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 400px;\"/></center>\n",
    "<center><img src=\"img/fa_avg4.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 400px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* For optimal Average reward value Functions\n",
    "<center><img src=\"img/fa_avg2.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 400px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Contrast with Discounted Value functions:\n",
    "\n",
    "* There is also a differential form of the two TD errors:\n",
    "\n",
    "* For disconted value functions\n",
    "<center><img src=\"img/fa_avg7.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 400px;\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* For Average reward value Functions\n",
    "<center><img src=\"img/fa_avg8.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 400px;\"/></center>\n",
    "* $\\bar{R}$ is an estimate of the average reward $\\eta(\\pi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Differential Semi-gradient Sarsa for Control\n",
    "<br>\n",
    "\n",
    "<center><img src=\"img/fa_avg9.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example from Operations:\n",
    "### An Access-Control Queuing Task\n",
    "\n",
    "* A set of k servers. \n",
    "\n",
    "* Customers of four different priorities arrive at a single queue.\n",
    "  * Reward of 1,2,4,8 respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  \n",
    "* In each time step, the customer at the head of the queue is\n",
    "  * accepted (assigned to one of the servers) or \n",
    "  * rejected (removed from the queue, with a reward of zero). \n",
    "  \n",
    "* On the next time step the next customer in the queue is considered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The queue never empties\n",
    "  * the priorities of the customers are equally randomly distributed. \n",
    "\n",
    "* A customer can not be served if there is no free server;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Each busy server becomes free with probability p on each time step.\n",
    "\n",
    "* **Task: Decide on each step whether to accept or reject the next customer,on the basis of**\n",
    "  * **His priority and** \n",
    "  * **The number of free servers,**<br>\n",
    "  **So as to maximize long-term reward without discounting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example from Operations:\n",
    "### An Access-Control Queuing Task\n",
    "<center><img src=\"img/fa_prob7.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 500px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example from Operations:\n",
    "### An Access-Control Queuing Task\n",
    "<center><img src=\"img/fa_prob8.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 800px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Futility of Discounting in continuous Approximation problems\n",
    "\n",
    "#### Crux: \n",
    "* An infinite sequence of returns with no beginning or end, and no clearly identified states.  \n",
    "* it turns out that the average of the discounted returns is proportional to the average reward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* average of the discounted returns is always $\\eta(\\pi)/(1 − \\gamma)$\n",
    "* ** The ordering of all policies in the average discounted return setting would be exactly the same as in the average-reward setting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* The discounted case is still pertinent, or at least possible, for the episodic case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Summary</center>\n",
    "\n",
    "* To add"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

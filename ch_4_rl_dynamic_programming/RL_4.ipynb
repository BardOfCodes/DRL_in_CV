{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Dynamic Programming Methods</center>\n",
    "### <center> Reference: Chapter 4, Sutton and Barto</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Contents</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Recap: What is Dynamic Programming?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Planning by DP in MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Iterative Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Example: Gridworld (Policy Evaluation and Policy Improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Synchronous Dynamic Programming Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Asynchronous Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Full-Width Backups/Sample Backups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Recap: What is Dynamic Programming</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Dynamic** sequential or temporal component to the problem\n",
    "\n",
    "**Programming** optimising a “program”, i.e. a policy\n",
    "\n",
    "\n",
    "* A method for solving complex problems\n",
    "* By breaking them down into subproblems\n",
    "    * Solve the subproblems\n",
    "    * Combine solutions to subproblems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dynamic programming - Use Cases\n",
    "* Scheduling algorithms\n",
    "* String algorithms (e.g. sequence alignment)\n",
    "* Graph algorithms (e.g. shortest path algorithms)\n",
    "* Graphical models (e.g. Viterbi algorithm)\n",
    "* Bioinformatics (e.g. lattice models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Requirements for Dynamic Programming\n",
    "\n",
    "Dynamic Programming is a very general solution method for problems which have two properties:\n",
    "* **Optimal substructure**\n",
    "    * Principle of optimality applies\n",
    "    * Optimal solution can be decomposed into subproblems\n",
    "* **Overlapping subproblems**\n",
    "    * Subproblems recur many times\n",
    "    * Solutions can be cached and reused\n",
    "* Markov decision processes satisfy both properties\n",
    "    * Bellman equation gives recursive decomposition\n",
    "    * Value function stores and reuses solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Planning by DP in MDP </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Dynamic programming assumes full knowledge of the MDP\n",
    "* It is used for planning in an MDP \n",
    "* For prediction:\n",
    "    * Input: MDP $< S, A, P, R, γ>$ and policy $π$\n",
    "    * Output: value function $v_π$\n",
    "* Or for control:\n",
    "    * Input: MDP $<S, A, P, R, γ>$\n",
    "    * Output: optimal value function $v_∗$ and optimal policy $π_∗$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Iterative Policy Evaluation </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/aaa.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/b.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pseudocode: Policy Evaluation\n",
    "<center><img src=\"img/s11.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Policy Improvement </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Policy Improvement Theorem\n",
    "\n",
    "Let $\\pi$ and $\\pi'$ be any pair of deterministic policies such that, for all s $\\epsilon$ S,\n",
    "$$q_\\pi(s,\\pi'(s)) \\geq v_{\\pi}(s)$$\n",
    "Then the policy $ \\pi '$ must be as good as, or better than $\\pi$. That is it must obtain greater or equal expected return from all states s $\\epsilon$ S:\n",
    "$$v_{\\pi'}(s) \\geq v_{\\pi}(s)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/d.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/e.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Example: Gridworld (Policy Evaluation and Policy Improvement) </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/e1.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/e2.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/e3.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Policy Iteration </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/p1.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pseudocode: Policy Iteration\n",
    "\n",
    "<center><img src=\"img/s2.png\" alt=\"RewardHypothesis\" style=\"width: 900px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Modified Policy Iteration\n",
    "\n",
    "* Does policy evaluation need to converge to $v_π$ ?\n",
    "* Or should we introduce a stopping condition\n",
    "    e.g. $\\epsilon$-convergence of value function\n",
    "* Or simply stop after k iterations of iterative policy evaluation?\n",
    "* For example, in the small gridworld k = 3 was sufficient to achieve optimal policy\n",
    "* Why not update policy every iteration? i.e. stop after k = 1\n",
    "* This is equivalent to **value iteration** (next section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generalised Policy Iteration\n",
    "<center><img src=\"img/p2.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Value Iteration </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Principle of Optimality\n",
    "\n",
    "Any optimal policy can be subdivided into two components:\n",
    "* An optimal first action $A_∗$\n",
    "* Followed by an optimal policy from successor state S'\n",
    "<center><img src=\"img/v1.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deterministic Value Iteration\n",
    "\n",
    "<center><img src=\"img/v2.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pseudocode: Value Iteration\n",
    "<center><img src=\"img/s3.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Shortest Path\n",
    "<center><img src=\"img/v3.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Value Iteration\n",
    "\n",
    "<center><img src=\"img/sa.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n",
    "<center><img src=\"img/sb.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/v5.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Synchronous Dynamic Programming Algorithm </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/s1.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Asynchronous Dynamic Programming Methods </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* DP methods described so far used synchronous backups\n",
    "* i.e. all states are backed up in parallel\n",
    "* Asynchronous DP backs up states individually, in any order\n",
    "* For each selected state, apply the appropriate backup\n",
    "* Can significantly reduce computation\n",
    "* Guaranteed to converge if all states continue to be selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Three simple ideas for asynchronous dynamic programming:\n",
    "\n",
    "* In-place dynamic programming\n",
    "* Prioritised sweeping\n",
    "* Real-time dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Full-Width Backup/ Sample Backup </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Full-Width Backup\n",
    "<center><img src=\"img/b1.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sample Backup\n",
    "\n",
    "<center><img src=\"img/b2.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

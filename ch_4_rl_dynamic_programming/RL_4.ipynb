{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Dynamic Programming Methods</center>\n",
    "### <center> Reference: Chapter 4, Sutton and Barto</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Contents</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Recap: What is Dynamic Programming?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Planning by DP in MDP\n",
    "  * Iterative Policy Evaluation\n",
    "  * Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Example: Gridworld (Policy Evaluation and Policy Improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Control: Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Control: Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Synchronous Dynamic Programming Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Asynchronous Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Full-Width Backups/Sample Backups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Recap: What is Dynamic Programming</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Recap: What is Dynamic Programming</center>\n",
    "\n",
    "**Dynamic**: sequential or temporal component to the problem\n",
    "\n",
    "**Programming**: optimising a “program”, i.e. a policy\n",
    "\n",
    "\n",
    "* A method for solving complex problems\n",
    "* By breaking them down into subproblems\n",
    "    * Solve the subproblems\n",
    "    * Combine solutions to subproblems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Requirements for Dynamic Programming\n",
    "\n",
    "Dynamic Programming is a very general solution method for problems which have two properties:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Optimal substructure**\n",
    "    * Principle of optimality applies\n",
    "    * Optimal solution can be decomposed into subproblems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Overlapping subproblems**\n",
    "    * Subproblems recur many times\n",
    "    * Solutions can be cached and reused\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Examples of DP problems:\n",
    "\n",
    "* #### Travelling salesman problem:\n",
    "<center><img src=\"img/dp_ex_1.png\" alt=\"travellingsalesman\" style=\"width: 300px;\"/></center>\n",
    "\n",
    "  * Optimal substructure?\n",
    "  * Overlapping subproblems?\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Algorithms like Dijsktra's algorithm for solving the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* ### Other Use Cases:\n",
    "  * String Sequence alignment\n",
    "  * Scheduling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DP in Markov Decision Process\n",
    "\n",
    "* Markov decision processes satisfy both properties\n",
    "    * Bellman equation gives recursive decomposition\n",
    "    * Value function stores and reuses solutions\n",
    "\n",
    "* Dynamic programming (DP) in MDP world refers to a collection of algorithms that can be used to compute optimal policies \n",
    "  * given a perfect model of the environment as a Markov decision process (MDP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Essential foundation for the understanding of the other RL methods\n",
    "* All other RL methods can be viewed as attempts to achieve much the same effect as DP, only with \n",
    "  * less computation and\n",
    "  * without assuming a perfect model of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Assumption:\n",
    "\n",
    "We assume that the environment is a finite MDP:\n",
    "* Finite Set of States\n",
    "* Finite Set of Actions\n",
    "* Finite Set of Rewards\n",
    "\n",
    "DP possible in Continuous Spaces as well, but more complex.(Will be seen in future Chapters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center>The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Planning by DP in MDP </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Dynamic programming assumes full knowledge of the MDP\n",
    "* It is used for planning in an MDP \n",
    "* For prediction:\n",
    "    * Input: MDP $< S, A, P, R, γ>$ and policy $π$\n",
    "    * Output: value function $v_π$\n",
    "* Or for control:\n",
    "    * Input: MDP $<S, A, P, R, γ>$\n",
    "    * Output: optimal value function $v_∗$ and optimal policy $π_∗$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Iterative Policy Evaluation </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/aaa.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/b.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pseudocode: Policy Evaluation\n",
    "<center><img src=\"img/s11.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Policy Improvement </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Policy Improvement Theorem\n",
    "\n",
    "Let $\\pi$ and $\\pi'$ be any pair of deterministic policies such that, for all s $\\epsilon$ S,\n",
    "$$q_\\pi(s,\\pi'(s)) \\geq v_{\\pi}(s)$$\n",
    "Then the policy $ \\pi '$ must be as good as, or better than $\\pi$. That is it must obtain greater or equal expected return from all states s $\\epsilon$ S:\n",
    "$$v_{\\pi'}(s) \\geq v_{\\pi}(s)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/d.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/e.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Example: Gridworld (Policy Evaluation and Policy Improvement) </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/e1.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/e2.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/e3.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Policy Iteration </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/p1.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pseudocode: Policy Iteration\n",
    "\n",
    "<center><img src=\"img/s2.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Modified Policy Iteration\n",
    "\n",
    "* Does policy evaluation need to converge to $v_π$ ?\n",
    "* Or should we introduce a stopping condition\n",
    "    e.g. $\\epsilon$-convergence of value function\n",
    "* Or simply stop after k iterations of iterative policy evaluation?\n",
    "* For example, in the small gridworld k = 3 was sufficient to achieve optimal policy\n",
    "* Why not update policy every iteration? i.e. stop after k = 1\n",
    "* This is equivalent to **value iteration** (next section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generalised Policy Iteration\n",
    "<center><img src=\"img/p2.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Value Iteration </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Principle of Optimality\n",
    "\n",
    "Any optimal policy can be subdivided into two components:\n",
    "* An optimal first action $A_∗$\n",
    "* Followed by an optimal policy from successor state S'\n",
    "<center><img src=\"img/v1.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deterministic Value Iteration\n",
    "\n",
    "<center><img src=\"img/v2.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pseudocode: Value Iteration\n",
    "<center><img src=\"img/s3.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Shortest Path\n",
    "<center><img src=\"img/v3.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Value Iteration\n",
    "\n",
    "<center><img src=\"img/sa.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n",
    "<center><img src=\"img/sb.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/v5.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Synchronous Dynamic Programming Algorithm </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/s1.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Problems with Synchronous DP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Involve operations over the entire state set of the MDP:\n",
    "  * Require sweeps of the state set.\n",
    "  * Even a single sweep can be prohibitively expensive.\n",
    "  * Game of backgammon has over $10^{20}$ states.\n",
    "  * value iteration backup at a million states per second ==> a thousand years to complete a single sweep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Algorithm gets locked into a long sweep before it can make progress improving a policy.\n",
    "  * take advantage of latest updates to backups so as to improve the algorithm’s rate of progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Skip backing up some states entirely if they are not relevant to optimal behavior.\n",
    "  * Update States related to the optimal behaviour only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Asynchronous Dynamic Programming Methods </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* DP methods described so far used synchronous backups\n",
    "* i.e. all states are backed up in parallel\n",
    "* Asynchronous DP backs up states individually, in any order\n",
    "* For each selected state, apply the appropriate backup\n",
    "* Can significantly reduce computation\n",
    "* Guaranteed to converge if all states continue to be selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Three simple ideas for asynchronous dynamic programming:\n",
    "\n",
    "* In-place dynamic programming\n",
    "* Prioritised sweeping\n",
    "* Real-time dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## In Place Dynamic Programming\n",
    "\n",
    "<center><img src=\"img/async_1.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritised Sweeping\n",
    "\n",
    "<center><img src=\"img/async_2.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time Dynamic Programming\n",
    "\n",
    "<center><img src=\"img/async_3.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Full-Width Backup/ Sample Backup </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Full-Width Backup\n",
    "<center><img src=\"img/b1.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sample Backup\n",
    "\n",
    "<center><img src=\"img/b2.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Summary</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Familiarity with the basic ideas and algorithms of dynamic programming in solving finite MDPs.\n",
    "  * Policy Evaluation\n",
    "  * Policy Improvement\n",
    "* Two Control Algorithms with DP\n",
    "  * Policy Iteration\n",
    "  * Value Iteration\n",
    "* Synchronous Vs Asynchronous Algorithms for implementing DP in RL\n",
    "  * In-place dynamic programming, Prioritised sweeping, Real-time dynamic programming"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

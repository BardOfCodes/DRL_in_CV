{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Dynamic Programming Methods</center>\n",
    "### <center> Reference: Chapter 4, Sutton and Barto</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Contents</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Recap: What is Dynamic Programming?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Planning by DP in MDP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Prediction\n",
    "    * Iterative Policy Evaluation\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Policy Improvement Theorem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Example: Gridworld (Policy Evaluation and Policy Improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Control\n",
    "    * Policy Iteration\n",
    "    * Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Synchronous vs Asynchronous Dynamic Programming Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Full-Width Backups/Sample Backups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Recap: What is Dynamic Programming</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Recap: What is Dynamic Programming</center>\n",
    "\n",
    "**Dynamic**: sequential or temporal component to the problem\n",
    "\n",
    "**Programming**: optimising a “program”, i.e. a policy\n",
    "\n",
    "\n",
    "* A method for solving complex problems\n",
    "* By breaking them down into subproblems\n",
    "    * Solve the subproblems\n",
    "    * Combine solutions to subproblems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Requirements for Dynamic Programming\n",
    "\n",
    "Dynamic Programming is a very general solution method for problems which have two properties:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Optimal substructure**\n",
    "    * Principle of optimality applies\n",
    "    * Optimal solution can be decomposed into subproblems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Overlapping subproblems**\n",
    "    * Subproblems recur many times\n",
    "    * Solutions can be cached and reused\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Examples of DP problems:\n",
    "\n",
    "* #### Travelling salesman problem:\n",
    "<center><img src=\"img/dp_ex_1.PNG\" alt=\"travellingsalesman\" style=\"width: 300px;\"/></center>\n",
    "\n",
    "  * Optimal substructure?\n",
    "  * Overlapping subproblems?\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Algorithms like Dijsktra's algorithm for solving the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* ### Other Use Cases:\n",
    "  * String Sequence alignment\n",
    "  * Scheduling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DP in Markov Decision Process\n",
    "\n",
    "* Markov decision processes satisfy both properties\n",
    "    * Bellman equation gives recursive decomposition\n",
    "    * Value function stores and reuses solutions\n",
    "\n",
    "* Dynamic programming (DP) in MDP world refers to a collection of algorithms that can be used to compute optimal policies \n",
    "  * given a perfect model of the environment as a Markov decision process (MDP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Essential foundation for the understanding of the other RL methods\n",
    "* All other RL methods can be viewed as attempts to achieve much the same effect as DP, only with \n",
    "  * less computation and\n",
    "  * without assuming a perfect model of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Assumption:\n",
    "\n",
    "We assume that the environment is a finite MDP:\n",
    "* Finite Set of States\n",
    "* Finite Set of Actions\n",
    "* Finite Set of Rewards\n",
    "\n",
    "DP possible in Continuous Spaces as well, but more complex.(Will be seen in future Chapters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center>The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Planning by DP in MDP </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Dynamic programming assumes full knowledge of the MDP\n",
    "* It is used for planning in an MDP \n",
    "* For prediction:\n",
    "    * Input: MDP $< S, A, P, R, γ>$ and policy $π$\n",
    "    * Output: value function $v_π$\n",
    "* Or for control:\n",
    "    * Input: MDP $<S, A, P, R, γ>$\n",
    "    * Output: optimal value function $v_∗$ and optimal policy $π_∗$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Iterative Policy Evaluation </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/aaa.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/b.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pseudocode: Policy Evaluation\n",
    "<center><img src=\"img/s11.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Policy Improvement </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Policy Improvement Theorem\n",
    "\n",
    "Let $\\pi$ and $\\pi'$ be any pair of deterministic policies such that, for all s $\\epsilon$ S,\n",
    "$$q_\\pi(s,\\pi'(s)) \\geq v_{\\pi}(s)$$\n",
    "Then the policy $ \\pi '$ must be as good as, or better than $\\pi$. That is it must obtain greater or equal expected return from all states s $\\epsilon$ S:\n",
    "$$v_{\\pi'}(s) \\geq v_{\\pi}(s)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/d.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/e.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Example: Gridworld (Policy Evaluation and Policy Improvement) </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/e1.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/e2.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/e3.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Policy Iteration </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/p1.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pseudocode: Policy Iteration\n",
    "\n",
    "<center><img src=\"img/s2.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Modified Policy Iteration\n",
    "\n",
    "* Does policy evaluation need to converge to $v_π$ ?\n",
    "* Or should we introduce a stopping condition\n",
    "    e.g. $\\epsilon$-convergence of value function\n",
    "* Or simply stop after k iterations of iterative policy evaluation?\n",
    "* For example, in the small gridworld k = 3 was sufficient to achieve optimal policy\n",
    "* Why not update policy every iteration? i.e. stop after k = 1\n",
    "* This is equivalent to **value iteration** (next section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generalised Policy Iteration\n",
    "<center><img src=\"img/p2.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Value Iteration </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Principle of Optimality\n",
    "\n",
    "Any optimal policy can be subdivided into two components:\n",
    "* An optimal first action $A_∗$\n",
    "* Followed by an optimal policy from successor state S'\n",
    "<center><img src=\"img/v1.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deterministic Value Iteration\n",
    "\n",
    "<center><img src=\"img/v2.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pseudocode: Value Iteration\n",
    "<center><img src=\"img/s3.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Shortest Path\n",
    "<center><img src=\"img/v3.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Value Iteration\n",
    "\n",
    "<center><img src=\"img/sa.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n",
    "<center><img src=\"img/sb.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/v5.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Synchronous Dynamic Programming Algorithm </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/s1.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Problems with Synchronous DP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Involve operations over the entire state set of the MDP:\n",
    "  * Require sweeps of the state set.\n",
    "  * Even a single sweep can be prohibitively expensive.\n",
    "  * Game of backgammon has over $10^{20}$ states.\n",
    "  * value iteration backup at a million states per second ==> a thousand years to complete a single sweep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Algorithm gets locked into a long sweep before it can make progress improving a policy.\n",
    "  * take advantage of latest updates to backups so as to improve the algorithm’s rate of progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Skip backing up some states entirely if they are not relevant to optimal behavior.\n",
    "  * Update States related to the optimal behaviour only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Asynchronous Dynamic Programming Methods </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* DP methods described so far used synchronous backups\n",
    "* i.e. all states are backed up in parallel\n",
    "* Asynchronous DP backs up states individually, in any order\n",
    "* For each selected state, apply the appropriate backup\n",
    "* Can significantly reduce computation\n",
    "* Guaranteed to converge if all states continue to be selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Three simple ideas for asynchronous dynamic programming:\n",
    "\n",
    "* In-place dynamic programming\n",
    "* Prioritised sweeping\n",
    "* Real-time dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## In Place Dynamic Programming\n",
    "\n",
    "<center><img src=\"img/async_1.PNG\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Prioritised Sweeping\n",
    "\n",
    "<center><img src=\"img/async_2.PNG\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Real-time Dynamic Programming\n",
    "\n",
    "<center><img src=\"img/async_3.PNG\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Full-Width Backup/ Sample Backup </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Full-Width Backup\n",
    "<center><img src=\"img/b1.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sample Backup\n",
    "\n",
    "<center><img src=\"img/b2.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Summary</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Familiarity with the basic ideas and algorithms of dynamic programming in solving finite MDPs.\n",
    "  * Policy Evaluation\n",
    "  * Policy Improvement\n",
    "* Two Control Algorithms with DP\n",
    "  * Policy Iteration\n",
    "  * Value Iteration\n",
    "* Synchronous Vs Asynchronous Algorithms for implementing DP in RL\n",
    "  * In-place dynamic programming, Prioritised sweeping, Real-time dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Extra Material</center>\n",
    "\n",
    "1) Proof of convergence for value iteration and policy iteration.\n",
    "\n",
    "2) Examples of Asynchronous DP in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Convergence of presented algorithms</center>\n",
    "\n",
    "1) How do we know that value iteration converges to the optimal values for the given policy $v_*$?\n",
    "\n",
    "2) Or that iterative policy evaluation converges to $v_\\pi$?\n",
    "\n",
    "3) Is the solution unique?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Basic Components of Proof:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* **Value Function Space**\n",
    "  * the vector space $V$ over value functions\n",
    "  * There are |S| dimensions\n",
    "  * Each point in this space fully specifies a value function v(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Value Function $\\infty$-Norm**\n",
    "  * distance between state-value functions $u$ and $v$ by the $\\infty$-norm  \n",
    "  * $$ ||u-v ||_{\\infty} = max_{s \\in S}|u(s) - v(s)|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Basic Components of Proof:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Contraction Mapping**\n",
    "  * A contraction mapping, on a metric space $(M,d)$ is a function $f$ from $M$ to itself, with the property that there is some nonnegative real number $0 \\leq  k \\le 1$ such that for all $x$ and $y$ in $M$,\n",
    "  * $$ d(f(x),f(y)) \\leq k d(x,y) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Contraction mapping Theorem**\n",
    "<center><img src=\"img/contr_1.PNG\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Expectation Backup is a Contraction\n",
    "<center><img src=\"img/b.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Expectation Backup is a Contraction\n",
    "<center><img src=\"img/contr_2.PNG\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Optimality Backup is a Contraction\n",
    "<center><img src=\"img/v5.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Optimality Backup is a Contraction\n",
    "<center><img src=\"img/contr_3.PNG\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hence,\n",
    "\n",
    "* The Bellman expectation operator $T_\\pi$ has a unique fixed point.\n",
    "* $v_\\pi$ is a fixed point of $T\\pi$ (by Bellman expectation equation)\n",
    "* By contraction mapping theorem,\n",
    "  * Iterative policy evaluation converges on $v_\\pi$\n",
    "  * Policy iteration converges on $v_∗$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Similarly,**\n",
    "\n",
    "* The Bellman optimality operator $T_*$ has a unique fixed point.\n",
    "* $v_∗$ is a fixed point of $T_∗$ (by Bellman optimality equation)\n",
    "* By contraction mapping theorem,\n",
    "  * Value iteration converges on $v_∗$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Real Life Examples of Asynchronous DP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Real Life Examples of Asynchronous DP\n",
    "\n",
    "* Asynchronous DP Algorithms outlined previously:\n",
    "  * In-place dynamic programming\n",
    "  * Prioritised sweeping\n",
    "  * Real-time dynamic programming\n",
    "\n",
    "* We will look into one example on how prioritised Sweeping can help improve Policy Evaulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Prioritized Sweeping\n",
    "\n",
    "<center><img src=\"img/async_ex_1.PNG\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A 500 State MDP\n",
    "* 16 Terminal States, 8 black and 8 white."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The prediction problem is to estimate, for every non-terminal state, the long-term probability that it will terminate in a black, rather than a white, circle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"img/async_ex_2.PNG\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Results\n",
    "<center><img src=\"img/async_ex_3.PNG\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Results\n",
    "<center><img src=\"img/async_ex_4.PNG\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conclusion:\n",
    "* Model-free methods perform well in real time but make weak use of their data. \n",
    "* Classical methods make good use of their data but are often impractically slow. \n",
    "* **Techniques such as prioritized sweeping are interesting because they may be able to achieve both.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Example of In-place DP in Tutorial\n",
    "* An example similar to real-time DP will be covered in Chapter 5."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

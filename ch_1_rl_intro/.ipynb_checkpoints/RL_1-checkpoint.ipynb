{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Introduction to Reinforcement Learning</center>\n",
    "\n",
    "### <center> Reference: Chapter 1, Sutton and Barto </center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Contents</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* RL: Formal Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* RL vs Supervised Learning vs Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Important RL Perspectives\n",
    "    * Goal (Reward Hypothesis)\n",
    "    * Sequential Decision Making Problem\n",
    "    * Interaction between Agent and Environment\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Components of RL Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* RL Problems: Learning and Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Prediction and Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>RL Formal Definition</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " \"Reinforcement learning is the problem of getting an agent to act in the world so as to maximize its rewards. For example, consider teaching a dog a new trick: you cannot tell it what to do, but you can reward/punish it if it does the right/wrong thing. It has to figure out what it did that made it get the reward/punishment, which is known as the credit assignment problem.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/1.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/2.png\" alt=\"Example2\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>RL vs Supervised Learning vs Unsupervised Learning</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "1) A human builds a classifier based on input and output data\n",
    "\n",
    "2) That classifier is trained with a training set of data\n",
    "\n",
    "3) That classifier is tested with a test set of data\n",
    "\n",
    "4) Deployment if the output is satisfactory\n",
    "\n",
    "To be used when, \"I know how to classify this data, I just need you(the classifier) to sort it.\"\n",
    "\n",
    "Point of method: To class labels or to produce real numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "\n",
    "1) A human builds an algorithm based on input data\n",
    "\n",
    "2) That algorithm is tested with a test set of data (in which the algorithm creates the classifier)\n",
    "\n",
    "3) Deployment if the classifier is satisfactory\n",
    "\n",
    "To be used when, \"I have no idea how to classify this data, can you(the algorithm) create a classifier for me?\"\n",
    "Point of method: To class labels or to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "\n",
    "1) A human builds an algorithm based on input data\n",
    "\n",
    "2) That algorithm presents a state dependent on the input data in which a user rewards or punishes the algorithm via the action the algorithm took, this continues over time\n",
    "\n",
    "3) That algorithm learns from the reward/punishment and updates itself, this continues\n",
    "\n",
    "4) It's always in production, it needs to learn real data to be able to present actions from states\n",
    "\n",
    "To be used when, \"I have no idea how to classify this data, can you classify this data and I'll give you a reward if it's correct or I'll punish you if it's not.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RL vs Supervised Learning\n",
    "* Training Examples\n",
    "    * Supervised Learning: Training Examples of the formNo training examples from a knowledgeable external supervisor (situation together with a label).\n",
    "    * RL: No such training examples.\n",
    "* Objective Functions\n",
    "    * Supervised Learning: Aim is to extrapolate, or generalize so that it acts correctly in situations not present in the training set. \n",
    "    * In RL, it is often impractical to obtain examples of desired behavior that are both correct and representative of all the situations and an agent must be able to learn from its own experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RL vs Unsupervised Learning\n",
    "* Unsupervised Learning is about finding structure hidden in collections of unlabeled data.\n",
    "* Uncovering structure in an agent’s experience can certainly be useful in reinforcement learning, but by itself does not address the reinforcement learning agent’s problem of maximizing a reward signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Examples of RL\n",
    "* Fly stunt manoeuvres in a helicopter\n",
    "* Defeat the world champion at Backgammon\n",
    "* Manage an investment portfolio\n",
    "* Control a power station\n",
    "* Make a humanoid robot walk\n",
    "* Play many different Atari games better than humans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Important RL Perspectives</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goal of RL (Reward Hypothesis)\n",
    "<center><img src=\"img/3.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reward Examples\n",
    "* stunt manoeuvres in a helicopter\n",
    "    * +ve reward for following desired trajectory\n",
    "    * −ve reward for crashing\n",
    "* Defeat the world champion at Backgammon\n",
    "    * +/−ve reward for winning/losing a game\n",
    "* Manage an investment portfolio\n",
    "    * +ve reward for each dollar in bank\n",
    "* Control a power station\n",
    "    * +ve reward for producing power\n",
    "    * −ve reward for exceeding safety thresholds\n",
    "* Make a humanoid robot walk\n",
    "    * +ve reward for forward motion\n",
    "    * −ve reward for falling over\n",
    "* Play many different Atari games better than humans\n",
    "    * +/−ve reward for increasing/decreasing score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequential Decision Making Problem\n",
    "* Goal: select actions to maximise total future reward\n",
    "* Actions may have long term consequences\n",
    "* Reward may be delayed\n",
    "* It may be better to sacrifice immediate reward to gain more long-term reward\n",
    "* Examples:\n",
    "    * A financial investment (may take months to mature)\n",
    "    * Refuelling a helicopter (might prevent a crash in several hours)\n",
    "    * Blocking opponent moves (might help winning chances many moves from now)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interaction between Agent and Environment\n",
    "<center><img src=\"img/6.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Interaction between Agent and Environment\n",
    "<center><img src=\"img/7.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## History and State\n",
    "<center><img src=\"img/8.png\" alt=\"HistoryandState\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Environment State\n",
    "<center><img src=\"img/9.png\" alt=\"HistoryandState\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Agent State\n",
    "<center><img src=\"img/10.png\" alt=\"HistoryandState\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Information State\n",
    "<center><img src=\"img/11.png\" alt=\"HistoryandState\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fully Observable Environment\n",
    "<center><img src=\"img/12.png\" alt=\"HistoryandState\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Partially Observable Environment\n",
    "<center><img src=\"img/13.png\" alt=\"HistoryandState\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Major Components of a RL Agent\n",
    "An RL agent may include one or more of these components:\n",
    "* **Policy**: agent’s behaviour function\n",
    "* **Value function**: how good is each state and/or action\n",
    "* **Model**: agent’s representation of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Policy\n",
    "* A policy is the agent’s behaviour\n",
    "* It is a map from state to action, e.g.\n",
    "* Deterministic policy: $a = π(s)$\n",
    "* Stochastic policy: $π(a|s) = P[A_t = a|S_t = s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Value function\n",
    "* Value function is a prediction of future reward\n",
    "* Used to evaluate the goodness/badness of states\n",
    "* And therefore to select between actions, e.g.\n",
    "$$v_π(s) = E_π[R _{t+1} + γ*R_{t+2} + γ^{2}*R_{t+3} + ... | S_t = s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model\n",
    "* A model predicts what the environment will do next\n",
    "* **Transitions**: P predicts the next state (i.e. dynamics)\n",
    "* **Rewards**: R predicts the next (immediate) reward, e.g.\n",
    "$$ P_{ss'} = P[S_{t+1} = s' | S_t = s, A_t = a]$$\n",
    "$$ R^{a}_{s} = E[R_{t+1} | S_t = s, A_t = a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Maze Example\n",
    "<center><img src=\"img/e1.png\" alt=\"HistoryandState\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Maze Example: Policy\n",
    "<center><img src=\"img/e2.png\" alt=\"HistoryandState\" style=\"width: 1000px;\"/></center>\n",
    "\n",
    "* Arrows represent policy $\\pi(s)$ for each state s. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Maze Example: Value Function\n",
    "<center><img src=\"img/e3.png\" alt=\"HistoryandState\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Maze Example: Model\n",
    "<center><img src=\"img/e4.png\" alt=\"HistoryandState\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning and Planning\n",
    "Two fundamental problems in sequential decision making:\n",
    "* Reinforcement Learning:\n",
    "    * The environment is initially unknown\n",
    "    * The agent interacts with the environment\n",
    "    * The agent improves its policy\n",
    "* Planning:\n",
    "    * A model of the environment is known\n",
    "    * The agent performs computations with its model (without any external interaction)\n",
    "    * The agent improves its policy a.k.a. deliberation, reasoning, introspection, pondering, thought, search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Exploration and Exploitation\n",
    "\n",
    "* Reinforcement learning is like trial-and-error learning\n",
    "\n",
    "* The agent should discover a good policy,\n",
    "  * From its experiences of the environment,\n",
    "  * Without losing too much reward along the way\n",
    "\n",
    "* **Exploration** finds more information about the environment\n",
    "\n",
    "* **Exploitation** exploits known information to maximise reward\n",
    "\n",
    "* It is usually important to explore as well as exploit (In Detail => Chapter 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Examples of Explortion and Exploitation\n",
    "\n",
    "* Restaurant Selection\n",
    "    * Exploitation Go to your favourite restaurant\n",
    "    * Exploration Try a new restaurant\n",
    "* Online Banner Advertisements\n",
    "    * Exploitation Show the most successful advert\n",
    "    * Exploration Show a different advert\n",
    "* Oil Drilling\n",
    "    * Exploitation Drill at the best known location\n",
    "    * Exploration Drill at a new location\n",
    "* Game Playing\n",
    "    * Exploitation Play the move you believe is best\n",
    "    * Exploration Play an experimental move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "* We got introduced to the basic terminologies of RL.\n",
    "\n",
    "\n",
    "* We got an intuition behind how an RL agent can solve problems.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

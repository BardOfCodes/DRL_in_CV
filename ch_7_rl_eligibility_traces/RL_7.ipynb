{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Unifying MC Methods and TD Methods</center>\n",
    "## <center>Bootstrapping, TD($\\lambda$) and Eligibility Traces</center>\n",
    "### <center> Reference: Chapter 7 and Chapter 12, Sutton and Barto</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* TD ($\\lambda$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Eligibility Traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Bootstrapping </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bootstrapping and Sampling\n",
    "<center><img src=\"img/1.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why do Bootstrapping?\n",
    "\n",
    "* Free from tyranny of the time step\n",
    "* Sometimes updates are required at every step of transition (one-step TD)\n",
    "    * Take into account every possible transition/ anything that has changed\n",
    "* Sometimes, it makes sense to only update every few stansitions (multi-step TD)\n",
    "    * Take into account significant/considerable changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## n-Step Prediction\n",
    "<center><img src=\"img/3.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## n-Step Return\n",
    "<center><img src=\"img/4.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## n-Step Prediction\n",
    "\n",
    "<center><img src=\"img/prediction.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## n-Step SARSA (On-Policy Control)\n",
    "\n",
    "* simply switch states for actions (state–action pairs) and then use an $\\epsilon$-greedy policy.\n",
    "* The n-step returns in terms of estimated action values:\n",
    "<center><img src=\"img/return.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n",
    "* Update made to a particular value of action-pair is as follows:\n",
    "<center><img src=\"img/update.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## n-Step SARSA (On-policy Control)\n",
    "\n",
    "<center><img src=\"img/sc1.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# n-Step SARSA Example\n",
    "\n",
    "<center><img src=\"img/eg1.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## n-Step SARSA (On-policy Control)\n",
    "\n",
    "<center><img src=\"img/nStepOnline.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## n-Step Off-Policy Control (with Importance Sampling)\n",
    "\n",
    "* learning the value function for one policy, π, while following another policy, μ\n",
    "* Often, π is the greedy policy for the current action-value-function estimate, and μ is a more exploratory policy, perhaps ε-greedy\n",
    "* we must take into account the difference between the two policies, using their relative probability of taking the actions that were taken\n",
    "* To measure this difference, we use the importance sampling ratio. \n",
    "* Only difference, that instead of measuring it for the entire episode, we measure it for n-steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The importance sampling ratio looks liks:\n",
    "<center><img src=\"img/return1.png\" alt=\"RewardHypothesis\" style=\"width: 300px;\"/></center>\n",
    "* The update Equatin looks like this:\n",
    "<center><img src=\"img/update1.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n",
    "\n",
    "<center><img src=\"img/sc1.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## n-Step Off-Policy Control (with Importance Sampling)\n",
    "<center><img src=\"img/importancesampling.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Off-Policy Control (w/o Importance Sampling => Tree BackUp Algorithm)\n",
    "* This backup is an alternating mix of sample transitions—from each action to the su bsequent state—and full backups—from each state we consider all the possible actions, their probability of occuring under π, and their action values.\n",
    "<center><img src=\"img/bp.png\" alt=\"RewardHypothesis\" style=\"width: 100px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Returns and updates are calculated as follows:\n",
    "<center><img src=\"img/eq1.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n",
    "<center><img src=\"img/eq2.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unified view of Reinforcement Learning\n",
    "<center><img src=\"img/2.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> TD($\\lambda$) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Averaging n-Step Returns\n",
    "<center><img src=\"img/5.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## $\\lambda$ Returns\n",
    "<center><img src=\"img/6.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TD(λ) Weighting Function\n",
    "<center><img src=\"img/7.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Forward-view TD(λ)\n",
    "<center><img src=\"img/8.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bckward-view TD(λ)\n",
    "<center><img src=\"img/9.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Eligibility Traces </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Eligibility Traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"img/10_1.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"img/10_2.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backward View TD(λ)\n",
    "<center><img src=\"img/11.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TD(λ) and TD(0)\n",
    "<center><img src=\"img/12.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TD(λ) and MC\n",
    "<center><img src=\"img/13.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MC and TD(1)\n",
    "<center><img src=\"img/14.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Telescoping in TD(1)\n",
    "<center><img src=\"img/15.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TD(λ) and TD(1)\n",
    "<center><img src=\"img/16.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Telescoping in TD(λ)\n",
    "<center><img src=\"img/17.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Forwards and Backwards TD(λ)\n",
    "<center><img src=\"img/18.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Offline Equivalence of Forward and Backward TD\n",
    "<center><img src=\"img/19.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Online Equivalence of Forward and Backward TD\n",
    "<center><img src=\"img/20.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Summary of Forward and Backward TD(λ)\n",
    "<center><img src=\"img/21.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

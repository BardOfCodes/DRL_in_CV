{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Policy Gradient Methods</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Table of Content\n",
    "\n",
    "1) Introduction: Policy Gradient vs the World, Advantages and Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "2) REINFORCE: Simplest Policy Gradient Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "3) Actor-Critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "4) Additional Enhancements to Actor-Critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# <center>Introduction</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Introduction </center>\n",
    "<center><img src=\"img/pg_1.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Value-Based Vs Policy-Based RL</center>\n",
    "<center><img src=\"img/pg_2.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Why Policy-Based RL</center>\n",
    "<center><img src=\"img/pg_3.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Can Learning Policy be easier than Learning Values of states?</center>\n",
    "* The policy may be a simpler function to approximate.\n",
    "* This is the simplest advantage that policy parameterization may have over action-value parameterization.\n",
    "\n",
    "Why?\n",
    "* Problems vary in the complexity of their policies and action-value functions. \n",
    "* For some, the action-value function is simpler and thus easier to approximate. \n",
    "* For others, the policy is simpler. \n",
    "\n",
    "\n",
    "** In the latter case a policy-based method will typically be faster to learn and yield a superior asymptotic policy.**\n",
    "\n",
    "Example: In Robotics Tasks with continuous Action space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Example of Stochastic Optimal Policy</center>\n",
    "<center><img src=\"img/pg_4.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Example of Stochastic Optimal Policy</center>\n",
    "<center><img src=\"img/pg_5.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Example of Stochastic Optimal Policy</center>\n",
    "<center><img src=\"img/pg_6.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Example of Stochastic Optimal Policy</center>\n",
    "<center><img src=\"img/pg_7.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why not use softmax of Action-Values for stochastic Policies?\n",
    "\n",
    "* This alone would not approach determinism if and when required.\n",
    "* The action-value estimates would differ by a finite amount, translating to specific probabilities other than 0 and 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If softmax + Temprature Paramenter T: T could be reduced over time to approach determinism.\n",
    "* However, in practice it would be difficult to choose the reduction schedule, or even the initial temperature, without more knowledge of the true action values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Whereas, Policy gradient is driven to produce the optimal stochastic policy.\n",
    "* If the optimal policy is deterministic, then the preferences of the optimal actions will be driven infinitely higher than all suboptimal actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>REINFORCE: Simplest Policy Gradient Method</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Quality Measure of Policy</center>\n",
    "<center><img src=\"img/pg_8.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Policy Optimisation</center>\n",
    "<center><img src=\"img/pg_9.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Gradient Ascent</center>\n",
    "<center><img src=\"img/pg_10.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Gradient Ascent - FDM</center>\n",
    "<center><img src=\"img/pg_11.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Analytic Gradient Ascent</center>\n",
    "<center><img src=\"img/pg_12.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Example- Softmax Policy</center>\n",
    "<center><img src=\"img/pg_13.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Example- Gaussian Policy</center>\n",
    "<center><img src=\"img/pg_14.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>One-step MDP</center>\n",
    "<center><img src=\"img/pg_15.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Policy Gradient Theorem</center>\n",
    "<center><img src=\"img/pg_16.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Policy Gradient Theorem-Proof</center>\n",
    "<center><img src=\"img/sutton_1.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Monte-Carlo Policy Gradient (REINFORCE)</center>\n",
    "<center><img src=\"img/pg_17.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> PuckWorld Example</center>\n",
    "<center><img src=\"img/pg_18.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>\n",
    "** DQN Demo [Reinforce.js](http://cs.stanford.edu/people/karpathy/reinforcejs/puckworld.html) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> REINFORCE with Baseline</center>\n",
    "\n",
    "* REINFORCE has good theoretical convergence properties. \n",
    "* The expected update over an episode is in the same direction as the performance gradient.\n",
    "* This assures: \n",
    "  * An improvement in expected performance for sufficiently small $\\alpha$, and \n",
    "  * Convergence to a local optimum under standard stochastic approximation conditions. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **However**,\n",
    "  * Monte Carlo method REINFORCE may be of high variance, and thus \n",
    "  * slow to learn.\n",
    "\n",
    "** Can we reduce the variance somehow? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> REINFORCE with Baseline</center>\n",
    "\n",
    "* The derivative of the quality $\\eta(\\theta)$ of policy network can be written as \n",
    "\n",
    "\n",
    "<center><img src=\"img/sutton_5.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 400px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Instead of using the Rewards/Action Vaules generated directly, we first compare it with a baseline:\n",
    "\n",
    "\n",
    "<center><img src=\"img/sutton_2.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 500px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The baseline can be any function, even a random variable, \n",
    "* **Only Condition**: Should not vary with action $a$; \n",
    "* **Any guesses why?**\n",
    "\n",
    "<center><img src=\"img/sutton_3.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Finally the Update equation becomes:\n",
    "<center><img src=\"img/sutton_4.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 500px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Actor Critic Methods</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Reducing Variance Using a Critic</center>\n",
    "<center><img src=\"img/pg_19.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Estimating the Action-Value Function</center>\n",
    "<center><img src=\"img/pg_20.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Action Value Actor Critic</center>\n",
    "<center><img src=\"img/pg_21.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Bias In Actor-Critic Algorithm</center>\n",
    "<center><img src=\"img/pg_22.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Compatible Function Approximation</center>\n",
    "<center><img src=\"img/pg_23.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Compatible Function Approximation- Proof</center>\n",
    "<center><img src=\"img/pg_24.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Additional Enhancements to Actor Critic</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Actor Critic with Baseline</center>\n",
    "<center><img src=\"img/pg_25.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Estimating the Advantage Function</center>\n",
    "<center><img src=\"img/pg_26.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Estimating the Advantage Function</center>\n",
    "<center><img src=\"img/pg_27.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Critics at different Time-Scales</center>\n",
    "<center><img src=\"img/pg_27.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Actors at different Time-Scale</center>\n",
    "<center><img src=\"img/pg_28.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Policy Gradient with Eligibility Traces</center>\n",
    "<center><img src=\"img/pg_29.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Policy Gradient with Eligibility Traces</center>\n",
    "<center><img src=\"img/pg_30.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Summary</center>\n",
    "<center><img src=\"img/pg_34.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

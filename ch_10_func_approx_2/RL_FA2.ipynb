{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Value Function Approximation</center>\n",
    "## <center>Part II</center>\n",
    "## <center>Reference: Sutton and Barto, Chapter 9-11</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Table of Contents</center>\n",
    "<br>\n",
    "\n",
    "* **Batch Reinforcement Methods**<br><br>\n",
    "\n",
    "* **Least Squares Policy Iteration(LSPI)**<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Batch Reinforcement Methods</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Batch Reinforcement Methods</center>\n",
    "<br>\n",
    "* Gradient descent is simple and appealing<br><br>\n",
    "* But it is not sample efficient<br><br>\n",
    "* Batch methods seek to find the best fitting value function<br><br>\n",
    "* Given the agent’s experience (“training data”)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Least Squares Prediction</center>\n",
    "\n",
    "<center><img src=\"img/fa2_slides1.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Stochastic Gradient Descent with Experience Replay</center>\n",
    "\n",
    "<center><img src=\"img/fa2_slides3.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Experience Replay in Deep Q-Networks (DQN)</center>\n",
    "\n",
    "<center><img src=\"img/fa2_slides4.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DQN Example and Code\n",
    "<center><img src=\"img/ex.png\" alt=\"Multi-armed Bandit\" style=\"width: 300px;\"/></center>\n",
    "\n",
    "#### CartPole Example\n",
    "The agent has to decide between two actions - moving the cart left or right - so that the pole attached to it stays upright.\n",
    "\n",
    "##### State Space\n",
    "State is the difference between the current screen patch and the previous one. This will allow the agent to take the velocity of the pole into account from one image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Q-network\n",
    "\n",
    "* Our model will be a convolutional neural network that takes in the difference between the current and previous screen patches. \n",
    "* It has two outputs, representing Q(s,left) and Q(s,right) (where s is the input to the network). \n",
    "* In effect, the network is trying to predict the quality of taking each action given the current input.\n",
    "<center><img src=\"img/2.png\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Replay Memory\n",
    "* Experience replay memory is used for training the DQN. \n",
    "* It stores the transitions that the agent observes, allowing us to reuse this data later. \n",
    "* By sampling from it randomly, the transitions that build up a batch are decorrelated. \n",
    "* It has been shown that this greatly stabilizes and improves the DQN training procedure.\n",
    "<center><img src=\"img/1.png\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Input Extraction\n",
    "\n",
    "How do we get the crop of the cart?\n",
    "\n",
    "<center><img src=\"img/3.png\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Selecting an Action\n",
    "\n",
    "This is done based on $\\epsilon$ greedy policy.\n",
    "<center><img src=\"img/4.png\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Training\n",
    "<center><img src=\"img/5.png\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>\n",
    "<center><img src=\"img/6.png\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/7.png\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Linear Least Squares Prediction</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Linear Least Squares Prediction</center>\n",
    "<br><br>\n",
    "* Experience replay finds least squares solution<br><br>\n",
    "* But it may take many iterations<br><br>\n",
    "* Using linear value function approximation $\\hat{v}(s, w) = x(s)^Tw$<br><br>\n",
    "* We can solve the least squares solution directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Linear Least Squares Prediction</center>\n",
    "\n",
    "<center><img src=\"img/fa2_slides5.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Linear Least Squares Prediction Algorithms</center>\n",
    "\n",
    "<center><img src=\"img/fa2_slides6.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Linear Least Squares Prediction Algorithms</center>\n",
    "\n",
    "<center><img src=\"img/fa2_slides7.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Least Squares Policy Iteration(LSPI)</center>\n",
    "\n",
    "<center><img src=\"img/fa2_slides8.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Least Squares Action-Value Function Approximation</center>\n",
    "\n",
    "<center><img src=\"img/fa2_slides9.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Least Squares Control</center>\n",
    "\n",
    "<center><img src=\"img/fa2_slides10.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Least Squares Q-Learning</center>\n",
    "\n",
    "<center><img src=\"img/fa2_slides11.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## <center>Least Squares Policy Iteration(LSPI) Algorithm</center>\n",
    "\n",
    "<center><img src=\"img/fa2_slides12.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Monte Carlo Methods</center>\n",
    "### <center> Reference: Chapter 5, Sutton and Barto</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Contents</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Monte Carlo Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Monte Carlo Estimation of Action Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Monte Carlo Control \n",
    "  * Monte Carlo Control without Exploring Starts \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Off-policy Prediction via Importance Sampling \n",
    "  * Incremental Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Off-Policy Monte Carlo Control \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Return-Specific Importance Sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Introduction</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Last Session we saw Dynamic Programming Approach to Solving MDPs\n",
    "  * Policy Iteration\n",
    "  * Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Problem**:\n",
    "  * Assumed complete knowledge of the environment.\n",
    "  * In many cases it is easy to generate experience sampled according to the desired probability distributions, \n",
    "  * but infeasible to obtain the distributions in explicit form.\n",
    "\n",
    "* Can we use Experience instead for solving the MDP?\n",
    "* How to solve the MDP when we do not have complete knowledge of the environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Monte Carlo Methods**: broad class of computational algorithms that rely on repeated random sampling to obtain numerical results.  \n",
    "\n",
    "* **In our case**: methods based on averaging complete returns\n",
    "\n",
    "* Previously we **computed** value functions from knowledge of the MDP, here we **learn** value functions from sample returns with the MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Monte-Carlo Prediction</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Monte-Carlo Policy Evaluation\n",
    "\n",
    "<center><img src=\"img/1.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## First-Visit Monte-Carlo Policy Evaluation\n",
    "\n",
    "<center><img src=\"img/2.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pseudocode: First-visit MC Policy Evaluation\n",
    "<center><img src=\"img/3.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Every-Visit Monte-Carlo Policy Evaluation\n",
    "<center><img src=\"img/4.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Diference?\n",
    "\n",
    "* \"These two Monte Carlo (MC) methods are very similar but have slightly different theoretical properties.\" - Sutton and Barto\n",
    "* Both have quadratic convergence to $v_\\pi(s)$ with respect to $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Incremental Mean\n",
    "<center><img src=\"img/5.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Incremental Monte-Carlo Updates\n",
    "<center><img src=\"img/6.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Important Facts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The estimates from Monte Carlo methods for each state are independent. \n",
    "  * The estimate for one state does not build upon the estimate of any other state,\n",
    "  * In other words, Monte Carlo methods do not bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We can learn value of only one or a subset of states. \n",
    "  * We can generate many sample episodes starting from the states of interest,\n",
    "  * Averaging returns from only these states ignoring all others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Monte Carlo Estimation of Action Values</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why compute action-values, instead of state-values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If a model is not available: particularly useful to estimate **action values q** rather than **state values v**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* With model, state values alone are sufficient to determine a policy; \n",
    "  * Simple one step looks ahead  and  \n",
    "  * chooses action which leads to the best combination of reward and next state value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Without model, however, state values alone are not sufficient. \n",
    "  * Require explicitly estimate the value of each action for the values to be useful\n",
    "  * Thus, one of our primary goals for Monte Carlo methods is to estimate $q_{∗}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Monte Carlo Control</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generalised Policy Iteration (Refresher) \n",
    "<center><img src=\"img/c1.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generalised Policy Iteration with Monte carlo Evaluation\n",
    "<center><img src=\"img/c2.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model-Free Policy Iteration Using Action-Value Function\n",
    "<center><img src=\"img/c3.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generalised Policy Iteration with Action-Value Function\n",
    "<center><img src=\"img/c4.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example of Greedy Action Selection\n",
    "<center><img src=\"img/c5.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Another Problem with Greedy in MC\n",
    "\n",
    "Problem => **Maintaining Exploration**\n",
    "\n",
    "Solution => **Assumption of Exploring Starts**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "### Exploring Starts\n",
    "* Every state-action pair has a nonzero probability of being selected as the start.\n",
    "* However, not always possible:\n",
    "  * particularly when learning directly from actual interaction with an environment.\n",
    "* But very Useful when possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MC On-Policy Control ES\n",
    "<center><img src=\"img/c6.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Assumptions in this -\n",
    "\n",
    "1) Infinite Episodes for Learning\n",
    "\n",
    "2) Exploring Starts Assumption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How to do without **Infinite Episodes for learning**?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* **Solution 1**\n",
    "  * Approximate $q_{\\pi_k}$ in each policy evaluation. \n",
    "  * Measurements and assumptions are made to obtain bounds on the magnitude and probability of error in the estimates, \n",
    "  * Sufficient steps are taken during each policy evaluation to assure that these bounds are sufficiently small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Solution 2**\n",
    "  * Forgo trying to complete policy evaluation before returning to policy improvement. \n",
    "  * On each evaluation step we move the value function toward $q_{\\pi_k}$, \n",
    "  * but we do not expect to actually get close except over many steps.\n",
    "  * Extreme case: Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How can you just assume Convergence??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\"** Convergence to this optimal fixed point seems inevitable as the changes to the action-value function decrease\n",
    "over time, but has not yet been formally proved. In our opinion, this is one of the\n",
    "most fundamental open theoretical questions in reinforcement learning** \" </center>\n",
    "\n",
    "- Sutton and Barto, Reinforcement Learning: An  Introduction(Draft)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How to do without **Exploring Starts Assumption**?\n",
    "\n",
    "* The only general way to ensure that all actions are selected infinitely often is that\n",
    "  * the agent to continue to select them. \n",
    "  * There are two approaches to ensuring this: **on-policy** methods and **off-policy** methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>On Policy vs Off Policy</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"img/c7.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Monte Carlo Control without Exploring Starts </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## $\\epsilon$-Greedy Exploration\n",
    "<center><img src=\"img/a1.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## $\\epsilon$-Greedy Policy Improvement\n",
    "<center><img src=\"img/a2.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Monte-Carlo Policy Iteration\n",
    "<center><img src=\"img/a3.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## On-Policy MC Control Without ES ($\\epsilon$ soft Policy)\n",
    "<center><img src=\"img/a6.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Off-policy Prediction via Importance Sampling </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Current Delimma: \n",
    "  * learn action values conditional on subsequent optimal behavior, \n",
    "  * but behave non-optimally in order to explore all actions (to find the optimal actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* On-policy approach is actually a compromise:\n",
    "  * learns action values not for the optimal policy, but for a near-optimal policy that still explores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Off-Policy Learning\n",
    "\n",
    "* Use two policies, \n",
    "  * one that is learned about and that becomes the optimal policy,\n",
    "  * one that is more exploratory and is used to generate behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  \n",
    "* Policy being learned about is called the **target** policy, \n",
    "\n",
    "* Policy used to generate behavior is called the **behavior** policy.\n",
    "\n",
    "* Hence, Learning is from data “off” the target policy, and the overall process is termed **off-policy learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Off-Policy Learning\n",
    "\n",
    "* As the data is due to a different policy, off-policy methods are often \n",
    "  * of greater variance and \n",
    "  * are slower to converge.\n",
    "\n",
    "* off-policy methods are more powerful and general.(On-policy is a subset of Off-policy)\n",
    "\n",
    "* They can be applied to learn from data generated by a conventional non-learning controller, or from a human expert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to evaluate using off-policy methods?\n",
    "\n",
    "* We wish to estimate $v_\\pi$ or $q_\\pi$, but \n",
    "  * Episodes following another policy $\\mu$, where $\\mu \\neq \\pi$. \n",
    "* Given $\\pi$ is the target policy, $\\mu$ is the behavior policy, \n",
    "\n",
    "* Both policies are considered fixed and given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "** Assumption of coverage**: $\\pi(a|s) > 0 \\quad implies \\quad \\mu(a|s) > 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* To evaluate action vaules following $\\pi$ given episodes from following policy $\\mu$ ,\n",
    "  * off-policy methods utilize importance sampling,\n",
    "  \n",
    "* **Importance sampling**: A general technique for estimating expected values under one distribution given samples from another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Importance sampling**: weight returns according to the relative probability of their trajectories under the target and behavior policies.\n",
    "  * This ratio is called  the **importance-sampling ratio**.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Importance Sampling\n",
    "\n",
    "* From $S_t$, the probability of the subsequent state–action trajectory, $A_t, S_{t+1}, A_{t+1},... , S_T$, \n",
    "  * occurring under any policy $\\pi$ is\n",
    "<center><img src=\"img/imp_sam_1.png\" alt=\"RewardHypothesis\" style=\"height: 70px;\"/></center>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The relative probability of the trajectory under the target and behavior policies (the importance-sampling ratio) is\n",
    "\n",
    "<center><img src=\"img/imp_sam_2.png\" alt=\"RewardHypothesis\" style=\"height: 70px;\"/></center>\n",
    "\n",
    "* Importance sampling ratio ends up depending only on the two policies and not on the MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Importance Sampling\n",
    "1) Ordinary Importance Sampling\n",
    "\n",
    "<center><img src=\"img/imp_sam_3.png\" alt=\"RewardHypothesis\" style=\"height: 70px;\"/></center>\n",
    "2) Weighted Importance Sampling\n",
    "<center><img src=\"img/imp_sam_4.png\" alt=\"RewardHypothesis\" style=\"height: 100px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The ordinary importance-sampling estimator is unbiased whereas the weighted importance-sampling estimator is biased\n",
    "\n",
    "* The variance of the ordinary importance-sampling estimator is in general unbounded, whereas in the weighted estimator,assuming bounded returns, the variance of the weighted importance-sampling estimator converges to zero.\n",
    "\n",
    "* Weighted estimator usually has dramatically lower variance and is strongly preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"img/imp_sam_5.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pseudocode: Off Policy MC Prediction\n",
    "<center><img src=\"img/a5.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pseudocode: Off Policy MC Control\n",
    "<center><img src=\"img/a7.png\" alt=\"RewardHypothesis\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>** Return-Specific Importance Sampling** - Refer Sutton and Barto, Reinforcement learning: An introduction(Draft).</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Monte Carlo Approach\n",
    "* On-Policy Prediction\n",
    "  * On-Policy Control (w/o Assumption: Exploring Starts)\n",
    "* Off-Policy Prediction\n",
    "  * Off-Policy Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* MC has several advantages over DP:\n",
    "  * Can learn V and Q directly from interaction with environment\n",
    "  * No need for full models\n",
    "  * No need to learn about ALL states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* MC methods provide an alternate policy evaluation process\n",
    "  * No bootstrapping, i.e. updating not dependent on successor states (as opposed to DP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One issue to watch for: maintaining sufficient exploration\n",
    "  * exploring starts, \n",
    "  * soft policies\n",
    "  * off-policy methods"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

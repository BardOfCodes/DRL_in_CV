{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Monte Carlo Methods</center>\n",
    "### <center> Reference: Chapter 5, Sutton and Barto</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Contents</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Monte Carlo Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Monte Carlo Estimation of Action Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Monte Carlo Control \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Monte Carlo Control without Exploring Starts \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Off-policy Prediction via Importance Sampling \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Incremental Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Off-Policy Monte Carlo Control \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Return-Specific Importance Sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Monte-Carlo Prediction</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Monte-Carlo Policy Evaluation\n",
    "\n",
    "<center><img src=\"img/1.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## First-Visit Monte-Carlo Policy Evaluation\n",
    "\n",
    "<center><img src=\"img/2.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pseudocode: First-visit MC Policy Evaluation\n",
    "<center><img src=\"img/3.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Every-Visit Monte-Carlo Policy Evaluation\n",
    "<center><img src=\"img/4.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Incremental Mean\n",
    "<center><img src=\"img/5.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Incremental Monte-Carlo Updates\n",
    "<center><img src=\"img/6.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Monte Carlo Estimation of Action Values</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why compute action-values, instead of state-values?\n",
    "\n",
    "If a model is not available, then it is particularly useful to estimate action values (q) rather than state values (v).\n",
    "\n",
    "With a model, state values alone are sufficient to determine a policy; one simply looks ahead one step and chooses \n",
    "whichever action leads to the best combination of reward and next state.\n",
    "\n",
    "Without a model, however, state values alone are not sufficient. One must explicitly estimate the value of each action in order for the values to be useful in\n",
    "suggesting a policy. Thus, one of our primary goals for Monte Carlo methods is to estimate $q_{∗}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Monte Carlo Control</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generalised Policy Iteration (Refresher) \n",
    "<center><img src=\"img/c1.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generalised Policy Iteration with Monte carlo Evaluation\n",
    "<center><img src=\"img/c2.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model-Free Policy Iteration Using Action-Value Function\n",
    "<center><img src=\"img/c3.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generalised Policy Iteration with Action-Value Function\n",
    "<center><img src=\"img/c4.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example of Greedy Action Selection\n",
    "<center><img src=\"img/c5.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Another Problem with Greedy in MC\n",
    "\n",
    "Problem => **Maintaining Exploration**\n",
    "\n",
    "Solution => **Assumption of Exploring Starts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MC On-Policy Control ES\n",
    "<center><img src=\"img/c6.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Assumptions in this -\n",
    "\n",
    "1) Infinite Episodes for Learning\n",
    "\n",
    "2) Exploring Starts Assumption\n",
    "\n",
    "How to do without **Exploring Starts Assumption**?\n",
    "\n",
    "The only general way to ensure that all actions are selected infinitely often is for\n",
    "the agent to continue to select them. There are two approaches to ensuring this,\n",
    "resulting in what we call on-policy methods and off-policy methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## On Policy vs Off Policy\n",
    "<center><img src=\"img/c7.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Monte Carlo Control without Exploring Starts </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## $\\epsilon$ Greedy Exploration$\n",
    "<center><img src=\"img/a1.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## $\\epsilon$ Greedy Policy Improvement\n",
    "<center><img src=\"img/a2.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Monte-Carlo Policy Iteration\n",
    "<center><img src=\"img/a3.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## On-Policy MC Control Without ES ($\\epsilon$ soft Policy)\n",
    "<center><img src=\"img/a6.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Off-policy Prediction via Importance Sampling </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is Off-Policy Learning?\n",
    "\n",
    "Use two policies, one that is learned about and that becomes the optimal policy,\n",
    "and one that is more exploratory and is used to generate behavior.\n",
    "The policy being learned about is called the target policy, and the policy used to\n",
    "generate behavior is called the behavior policy.\n",
    "In this case we say that learning is from data “off” the target policy, and the overall\n",
    "process is termed off-policy learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is Off Policy Prediction Problem?\n",
    "<center><img src=\"img/a4.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Importance Sampling\n",
    "1) Ordinary Importance Sampling\n",
    "\n",
    "2) Weighted Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pseudocode: Off Policy MC Prediction\n",
    "<center><img src=\"img/a5.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pseudocode: Off Policy MC Control\n",
    "<center><img src=\"img/a7.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Monte Carlo Approach\n",
    "* On-Policy Prediction\n",
    "* On-Policy Control (w/o Assumption: Exploring Starts)\n",
    "* Off-Policy Prediction\n",
    "* Off-Policy Control"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

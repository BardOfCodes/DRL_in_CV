{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Finite Markov Decision Processes</center>\n",
    "\n",
    "### <center> Reference: Chapter 3, Sutton and Barto </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Contents</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Why MDPs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Markov Property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Building Blocks of MDP\n",
    "    * Episodic vs Continuous Tasks\n",
    "    * State Transition Matrix\n",
    "    * Return\n",
    "    * Discount\n",
    "    * Value Function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* MDP Parameters\n",
    "    * Policy in MDP notations\n",
    "    * Value Functions in MDP notations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Bellman Expectation Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Bellman Optimal Equations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Why Markov Decision Process?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Markov decision processes formally **describe an environment** for reinforcement learning\n",
    "* Where the environment is **fully observable**\n",
    "* i.e. The **current state** completely characterises the process\n",
    "* Almost all RL problems can be formalised as MDPs, e.g.\n",
    "    * Optimal control primarily deals with continuous MDPs\n",
    "    * Partially observable problems can be converted into MDPs\n",
    "    * Bandits are MDPs with one state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Markov Property</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "“The future is independent of the past given the present”\n",
    "<center> <img src=\"img/1.png\" alt=\"MarkovProperty Definition\" style=\"width:700px;\"/> </center>\n",
    "\n",
    "* The state captures all relevant information from the history\n",
    "* Once the state is known, the history may be thrown away\n",
    "* i.e. The state is a sufficient statistic of the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Building Blocks of MDP</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Episodic vs Continuing Tasks\n",
    "\n",
    "### Episodic Tasks\n",
    "* Each episode ends in a special state called the terminal state, \n",
    "* Followed by a reset to a standard starting state or to a sample from a standard distribution of starting states. \n",
    "\n",
    "### Continuing Tasks\n",
    "\n",
    "* The agent–environment interaction does not break naturally into identifiable episodes.\n",
    "* It goes on continually without limit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unified Notation for Episodic and Continuous Tasks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Return for Episodic Tasks\n",
    "sum over a finite number of terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Return for Continuous Tasks \n",
    "sum over an infinite number of terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We need one convention to obtain a single notation that covers both episodic and continuing tasks.\n",
    "\n",
    "How to do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "These can be unified by considering episode termination to be the entering\n",
    "of a **special absorbing state** that **transitions only to itself** and that **generates only\n",
    "rewards of zero**. For example, consider the state transition diagram -\n",
    "<center><img src=\"img/unified.png\" alt=\"Matrix\" style=\"width: 1000px;\"/></center>\n",
    "Hence, return can be written as-\n",
    "<center><img src=\"img/return.png\" alt=\"Matrix\" style=\"width: 200px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## State Transition Matrix\n",
    "<center><img src=\"img/2.png\" alt=\"Matrix\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Return\n",
    "<center><img src=\"img/3.png\" alt=\"Matrix\" style=\"width: 1000px;\"/></center>\n",
    "* The discount $γ ∈ [0, 1]$ is the present value of future rewards\n",
    "* The value of receiving reward R after k + 1 time-steps is $γ^k R$.\n",
    "* This values immediate reward above delayed reward.\n",
    "    * $γ$ close to 0 leads to ”myopic” evaluation\n",
    "    * $γ$ close to 1 leads to ”far-sighted” evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Discount \n",
    "\n",
    "Most Markov reward and decision processes are discounted. Why?\n",
    "* Mathematically convenient to discount rewards\n",
    "* Avoids infinite returns in cyclic Markov processes\n",
    "* Uncertainty about the future may not be fully represented\n",
    "* If the reward is financial, immediate rewards may earn more interest than delayed rewards\n",
    "* Animal/human behaviour shows preference for immediate reward\n",
    "* It is sometimes possible to use undiscounted Markov reward processes (i.e. $γ = 1$), e.g. if all sequences terminate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Value Function\n",
    "The value function $v(s)$ gives the long-term value of state s\n",
    "<center><img src=\"img/5.png\" alt=\"Matrix\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>MDP Parameters</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A Markov decision process (MDP) is a Markov reward process with decisions. It is an environment in which all states are Markov.\n",
    "\n",
    "<center><img src=\"img/4.png\" alt=\"Matrix\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Policy in MDP notation\n",
    "<center><img src=\"img/6.png\" alt=\"Matrix\" style=\"width: 1000px;\"/></center>\n",
    "* A policy fully defines the behaviour of an agent\n",
    "* MDP policies depend on the current state (not the history)\n",
    "* i.e. Policies are **stationary** (time-independent),\n",
    "    $A_t ∼ π(·|S_t ), \\forall t > 0$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Policy in MDP notation\n",
    "Given a MDP $M = \\left \\langle S, A, P, R, \\gamma \\right \\rangle$ and a policy $\\pi$\n",
    "\n",
    "$$P_{s,s'}^{\\pi} = \\sum_{a \\epsilon A} \\pi(a|s) P_{ss'}^{a}$$\n",
    "$$R_{s}^{\\pi} = \\sum_{a \\epsilon A} \\pi(a|s) R_{s}^{a}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Recycling Robot\n",
    "<center><img src=\"img/robot.jpg\" alt=\"Matrix\" style=\"width: 100px;\"/></center>\n",
    "\n",
    "** Task: ** \n",
    "\n",
    "Collect Empty soda cans in office\n",
    "    \n",
    "** Sensors: **\n",
    "    \n",
    "1) Detector : For detecting cans\n",
    "    \n",
    "2) Arm + Gripper : To pick up and place can in onboard bin\n",
    "        \n",
    "** <center>How can we formulate this as a MDP?</center> **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** We first need to identify States (S), Actions (A) and Rewards (R) **\n",
    "\n",
    "** Actions: **\n",
    "    \n",
    "1) {Search} - Actively search for a can\n",
    "\n",
    "2) {Wait} - Remain stationary and wait for someone to bring a can. (Will lose less battery)\n",
    "\n",
    "3) {Recharge} - Head back home for recharging\n",
    "    \n",
    "** States: **\n",
    "    \n",
    "1) high - Battery is charged considerably well\n",
    "2) low - Battery is not charged\n",
    "    \n",
    "** Rewards: ** \n",
    "    \n",
    "1) zero most of the time\n",
    "\n",
    "2) become positive when the robot secures an empty can, \n",
    "\n",
    "3) negative if the battery runs all the way down\n",
    "    \n",
    "** <center>How can we now formulate this as a MDP?</center> **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transition Probabilities and Expected Rewards\n",
    "<center><img src=\"img/pic1.png\" alt=\"Matrix\" style=\"width: 500px;\"/></center>\n",
    "## Transition Graph\n",
    "<center><img src=\"img/pic2.png\" alt=\"Matrix\" style=\"width: 500px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Value Function in MDP notation\n",
    "<center><img src=\"img/7.png\" alt=\"Matrix\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Bellman Expectation Equation</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Expectation Equation\n",
    "<center><img src=\"img/b1.png\" alt=\"Matrix\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Expectation Equation for $V^\\pi$\n",
    "<center><img src=\"img/b2.png\" alt=\"Matrix\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Expectation Equation for $Q^\\pi$\n",
    "<center><img src=\"img/b3.png\" alt=\"Matrix\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Expectation Equation for $v_\\pi$\n",
    "<center><img src=\"img/b4.png\" alt=\"Matrix\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Expectation Equation for $q_\\pi$\n",
    "<center><img src=\"img/b5.png\" alt=\"Matrix\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Bellman Optimality Equation</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimal Value Function\n",
    "<center><img src=\"img/o.png\" alt=\"Matrix\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimal Policy\n",
    "<center><img src=\"img/o1.png\" alt=\"Matrix\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Finding an Optimal Policy\n",
    "<center><img src=\"img/o2.png\" alt=\"Matrix\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Optimality Equation for $v_{*}$\n",
    "<center><img src=\"img/op1.png\" alt=\"Matrix\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Optimality Equation for $Q_{*}$\n",
    "<center><img src=\"img/op2.png\" alt=\"Matrix\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Optimality Equatin for $V^{*}$\n",
    "<center><img src=\"img/op3.png\" alt=\"Matrix\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Optimality Equation for $Q^{*}$\n",
    "<center><img src=\"img/op4.png\" alt=\"Matrix\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "* We looked into the MDP formulation of a RL problem.\n",
    "* We looked into the formulation of Value functions.\n",
    "    * action-value pairs\n",
    "    * state-action pairs\n",
    "* Understood the motivation and necessity of Bellman Expectation Equations and Bellman Optimlality Equations"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Finite Markov Decision Processes</center>\n",
    "\n",
    "### <center> Reference: Chapter 3, Sutton and Barto </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Contents</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Building Blocks of MDP\n",
    "  * Markov Property\n",
    "  * Episodic vs Continuous Tasks\n",
    "  * State Transition Matrix\n",
    "  * Return\n",
    "  * Discount\n",
    "  * Value Function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* MDP Parameters\n",
    "    * Policy in MDP notations\n",
    "    * Value Functions in MDP notations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Bellman Expectation Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Bellman Optimal Equations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>The Agent Environment Interface</center>\n",
    "\n",
    "\n",
    "<center> <img src=\"img/agent_env.PNG\" alt=\"MarkovProperty Definition\" style=\"width:700px;\"/> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* At each time step t, \n",
    "  * the agent has some representation of the environment’s state, $S_t \\in S$, \n",
    "    * where S is the set of possible states,\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  * On basis of $S_t$,agent selects an action, $A_t \\in A(S_t)$, \n",
    "    * where $A(S_t)$ is the set of actions available in state $S_t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  * One time step later, as a consequence of its action, \n",
    "    * the agent receives a numerical reward, $R_{t+1} \\in R \\subset R$, and \n",
    "    * finds itself in a new state, $S_{t+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A very Flexible Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The time steps can be any unit, non-fixed intervals of real-time;\n",
    "  * Tracking: decision is per time step\n",
    "  * Inventory management RL agent: Based only on current inventory state, time for depletion may vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Actions can be low-level controls,or High-level Decisions\n",
    "  * The voltages applied to the motors of a robot arm\n",
    "  * Whether or not to have lunch or to go to graduate school."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The states can take a wide variety of forms:\n",
    "  * determined by low-level sensations, such as direct sensor readings, \n",
    "  * high-level and abstract, such as symbolic descriptions of objects in a room."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* State could be based on memory of past sensations or even be entirely mental or subjective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some Key Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Boundary between agent and environment:not necessarily the same as the physical boundary of a robot’s or animal’s body.\n",
    "  * **The agent–environment boundary represents the limit of the agent’s absolute control, not of its knowledge.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The reward signal Should communicate **what** you want it to achieve, not **how** you want it achieved.\n",
    "  * Reward is placed outside the agent, so that it cant explicitly change reward formulation without achieving the goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The state signal should not be expected to inform the agent of everything about the environment,\n",
    "  * The agent is not designed to be omniscient.\n",
    "  * State can contain, a copy of reward, part memory etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# <center>Why Markov Decision Process?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Markov decision processes formally **describe an environment** for reinforcement learning\n",
    "* Where the environment is **fully observable**\n",
    "* i.e. The **current state** completely characterises the process\n",
    "* Almost all RL problems can be formalised as MDPs, e.g.\n",
    "    * Optimal control primarily deals with continuous MDPs\n",
    "    * Partially observable problems can be converted into MDPs\n",
    "    * Bandits are MDPs with one state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Building Blocks of MDP</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Markov Property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "“The future is independent of the past given the present”\n",
    "<center> <img src=\"img/1.png\" alt=\"MarkovProperty Definition\" style=\"width:700px;\"/> </center>\n",
    "\n",
    "* The state captures all relevant information from the history\n",
    "* Once the state is known, the history may be thrown away\n",
    "* i.e. The state is a sufficient statistic of the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Markov State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A state signal that succeeds in retaining all relevant information is said to be Markov, or to have the Markov property.\n",
    "  * Checker Position\n",
    "  * Location and Velocity of an Cannon ball"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If an environment has the Markov property,\n",
    "  * one-step dynamics enable prediction of next state and expected next reward\n",
    "  * based on current Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In such situation, the best policy for choosing actions as a function of a Markov state,\n",
    "  * **is just as good as** the best policy for choosing actions as a function of complete histories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* When state signal is non-Markov\n",
    "  * We still approximate it as a markov process as,\n",
    "  * We want our state representation to:\n",
    "    * Be a good basis for predicting subsequent states,\n",
    "    * Be a good basis for predicting future rewards, and\n",
    "    * Be a good basis for selecting actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## State Transition Matrix\n",
    "<center><img src=\"img/2.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Markov Process\n",
    "<center><img src=\"img/slides_1.PNG\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example\n",
    "<center><img src=\"img/slides_2.PNG\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example of Episodes\n",
    "<center><img src=\"img/slides_3.PNG\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example of Transition Matrix\n",
    "<center><img src=\"img/slides_4.PNG\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Markov Reward Process\n",
    "<center><img src=\"img/slides_5.PNG\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example\n",
    "<center><img src=\"img/slides_6.PNG\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Return\n",
    "<center><img src=\"img/3.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>\n",
    "* The discount $γ ∈ [0, 1]$ is the present value of future rewards\n",
    "* The value of receiving reward R after k + 1 time-steps is $γ^k R$.\n",
    "* This values immediate reward above delayed reward.\n",
    "    * $γ$ close to 0 leads to ”myopic” evaluation\n",
    "    * $γ$ close to 1 leads to ”far-sighted” evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Discount \n",
    "\n",
    "Most Markov reward and decision processes are discounted. Why?\n",
    "* Mathematically convenient to discount rewards\n",
    "* Avoids infinite returns in cyclic Markov processes\n",
    "* Uncertainty about the future may not be fully represented\n",
    "* If the reward is financial, immediate rewards may earn more interest than delayed rewards\n",
    "* Animal/human behaviour shows preference for immediate reward\n",
    "* It is sometimes possible to use undiscounted Markov reward processes (i.e. $γ = 1$), e.g. if all sequences terminate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example\n",
    "<center><img src=\"img/slides_7.PNG\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Value Function\n",
    "The value function $v(s)$ gives the long-term value of state s\n",
    "<center><img src=\"img/5.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example\n",
    "<center><img src=\"img/slides_8.PNG\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example\n",
    "<center><img src=\"img/slides_9.PNG\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example\n",
    "<center><img src=\"img/slides_10.PNG\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Episodic vs Continuing Tasks\n",
    "\n",
    "### Episodic Tasks\n",
    "* Each episode ends in a special state called the terminal state, \n",
    "* Followed by a reset to a standard starting state or to a sample from a standard distribution of starting states. \n",
    "\n",
    "### Continuing Tasks\n",
    "\n",
    "* The agent–environment interaction does not break naturally into identifiable episodes.\n",
    "* It goes on continually without limit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unified Notation for Episodic and Continuous Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Return for Episodic Tasks\n",
    "sum over a finite number of terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Return for Continuous Tasks \n",
    "sum over an infinite number of terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We need one convention to obtain a single notation that covers both episodic and continuing tasks.\n",
    "\n",
    "How to do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "These can be unified by considering episode termination to be the entering\n",
    "of a **special absorbing state** that **transitions only to itself** and that **generates only\n",
    "rewards of zero**. For example, consider the state transition diagram -\n",
    "<center><img src=\"img/unified.png\" alt=\"Matrix\" style=\"width: 800px;\"/></center>\n",
    "Hence, return can be written as-\n",
    "<center><img src=\"img/return.png\" alt=\"Matrix\" style=\"width: 200px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Markov Decision Process</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Markov Decision Process</center>\n",
    "A Markov decision process (MDP) is a Markov reward process with decisions. It is an environment in which all states are Markov.\n",
    "\n",
    "<center><img src=\"img/4.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Policy in MDP notation\n",
    "<center><img src=\"img/6.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>\n",
    "* A policy fully defines the behaviour of an agent\n",
    "* MDP policies depend on the current state (not the history)\n",
    "* i.e. Policies are **stationary** (time-independent),\n",
    "    $A_t ∼ π(·|S_t ), \\forall t > 0$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Policy in MDP notation\n",
    "Given a MDP $M = \\left \\langle S, A, P, R, \\gamma \\right \\rangle$ and a policy $\\pi$\n",
    "\n",
    "$$P_{s,s'}^{\\pi} = \\sum_{a \\epsilon A} \\pi(a|s) P_{ss'}^{a}$$\n",
    "$$R_{s}^{\\pi} = \\sum_{a \\epsilon A} \\pi(a|s) R_{s}^{a}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Recycling Robot\n",
    "<center><img src=\"img/robot.jpg\" alt=\"Matrix\" style=\"width: 100px;\"/></center>\n",
    "\n",
    "** Task: ** \n",
    "\n",
    "Collect Empty soda cans in office\n",
    "    \n",
    "** Sensors: **\n",
    "    \n",
    "1) Detector : For detecting cans\n",
    "    \n",
    "2) Arm + Gripper : To pick up and place can in onboard bin\n",
    "        \n",
    "** <center>How can we formulate this as a MDP?</center> **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** We first need to identify States (S), Actions (A) and Rewards (R) **\n",
    "\n",
    "** Actions: **\n",
    "    \n",
    "1) {Search} - Actively search for a can\n",
    "\n",
    "2) {Wait} - Remain stationary and wait for someone to bring a can. (Will lose less battery)\n",
    "\n",
    "3) {Recharge} - Head back home for recharging\n",
    "    \n",
    "** States: **\n",
    "    \n",
    "1) high - Battery is charged considerably well\n",
    "2) low - Battery is not charged\n",
    "    \n",
    "** Rewards: ** \n",
    "    \n",
    "1) zero most of the time\n",
    "\n",
    "2) become positive when the robot secures an empty can, \n",
    "\n",
    "3) negative if the battery runs all the way down\n",
    "    \n",
    "** <center>How can we now formulate this as a MDP?</center> **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transition Probabilities and Expected Rewards\n",
    "<center><img src=\"img/pic1.png\" alt=\"Matrix\" style=\"width: 400px;\"/></center>\n",
    "\n",
    "## Transition Graph\n",
    "<center><img src=\"img/pic2.png\" alt=\"Matrix\" style=\"width: 500px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Value Function in MDP notation\n",
    "<center><img src=\"img/7.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Bellman Expectation Equation</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Expectation Equation\n",
    "<center><img src=\"img/b1.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Expectation Equation for $V^\\pi$\n",
    "<center><img src=\"img/b2.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Expectation Equation for $Q^\\pi$\n",
    "<center><img src=\"img/b3.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Expectation Equation for $v_\\pi$\n",
    "<center><img src=\"img/b4.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Expectation Equation for $q_\\pi$\n",
    "<center><img src=\"img/b5.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Bellman Optimality Equation</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimal Value Function\n",
    "<center><img src=\"img/o.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimal Policy\n",
    "<center><img src=\"img/o1.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Finding an Optimal Policy\n",
    "<center><img src=\"img/o2.png\" alt=\"Matrix\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Optimality Equation for $v_{*}$\n",
    "<center><img src=\"img/op1.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Optimality Equation for $Q_{*}$\n",
    "<center><img src=\"img/op2.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Optimality Equatin for $V^{*}$\n",
    "<center><img src=\"img/op3.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Optimality Equation for $Q^{*}$\n",
    "<center><img src=\"img/op4.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "* We looked into the MDP formulation of a RL problem.\n",
    "* We looked into the formulation of Value functions.\n",
    "    * action-value pairs\n",
    "    * state-action pairs\n",
    "* Understood the motivation and necessity of Bellman Expectation Equations and Bellman Optimlality Equations"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

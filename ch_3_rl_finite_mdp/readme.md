
# Finite Markov Decision Processes

### Reference: Chapter 3, Sutton and Barto

## Contents:

1) **Why MDPs?**

2) **Markov Property**

3) **Building Blocks of MDP**
* Episodic vs Continuous Tasks
* State Transition Matrix
* Return
* Discount
* Value Function

4) **MDP Parameters**
* Policy in MDP notations
* Value Functions in MDP notations

5) **Bellman Expectation Equations**

6) **Bellman Optimality Equations**



## Summary

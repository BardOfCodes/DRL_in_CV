{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Planning and learning with tabular methods</center>\n",
    "#### <center>Reference: Sutton and Barto Chapter 8</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Model Based RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) Dyna: Integrating Planning, Acting, and Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4) Prioritizing Sweeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "5) Planning as a part of action Selection(MCTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Introduction</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Model-Free RL\n",
    "  * No model\n",
    "  * Learn value function (and/or policy) from experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Model-Based RL\n",
    "  * Learn a model from experience\n",
    "  * Plan value function (and/or policy) from mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this Chapter we will learn how to **learn** a model directly from the environment and use **planning** to construct a value function/policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is a model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "A model of the environment we mean anything that an agent can use to predict how the environment will respond to its actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Some models produce a description of all possibilities and their probabilities; these we call **distribution** models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Other models produce just one of the possibilities, sampled according to the probabilities; these we call **sample** models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is planning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "We use the term Planning to refer to any computational process that takes a model as input and produces or improves a policy for interacting with the modeled environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "**state-space planning**- Planning is viewed primarily as a search through the state space for an optimal policy or path to a goal.\n",
    "\n",
    "\n",
    "**plan-space planning**-  Planning is instead viewed as a search through the space of plans.Plan-space planning includes evolutionary methods and “partial-order planning\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In state space planning, two fundamental ideas:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "1) All state-space planning methods involve computing value functions as a key intermediate step toward improving the policy, and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "2) They compute their value functions by backup operations applied to simulated experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"img/sslearning.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For eg: Dynamic Programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Difference between Planning and Learning:\n",
    "\n",
    "The difference is that whereas planning uses simulated experience generated by a model, learning methods use real experience generated by the environment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Model Based RL</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"img/modelbasedplanning.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Advantages**:\n",
    "  * Can efficiently learn model by supervised learning methods\n",
    "  * Can reason about model uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Disadvantages**:\n",
    "  * First learn a model, then construct a value function\n",
    "  * Two sources of approximation error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* We will assume state space $S$ and action space $A$ are known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* A model $M$ is a representation of an MDP $\\{S, A,P, R\\}$ parametrized by $\\eta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* So a model $M = \\{P_\\eta,R_\\eta\\}$, where \n",
    "  * $P_\\eta \\approx P$ represents state transitions\n",
    "  * and $R_\\eta \\approx R$ represents the rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* $$ S_{t+1} ~ P_\\eta(S_{t+1} | S_t, A_t)$$  $$ R_{t+1} = R_\\eta(R_{t+1} | S_t, A_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Typically assume conditional independence between state transitions and rewards:<br>\n",
    "$$\n",
    "P [S_{t+1}, R_{t+1} | St , At ] = P [S_{t+1} | St, At] P [R_{t+1} | St , At] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "**Goal**: estimate model $M_\\eta$ from experience $\\{S_1, A_1, R_2, ..., S_T \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* This is a supervised learning problem\n",
    "  * $S_1, A_1 \\rhd R_2, S_2$<br>\n",
    "  * $S_2, A_2 \\rhd R_3, S_3$<br>\n",
    "  * .<br>\n",
    "  * .<br>\n",
    "  * .<br>\n",
    "  * $S_{T−1}, A_{T−1} \\rhd R_T , S_T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Learning $s, a \\rhd r$ is a regression problem,\n",
    "* Learning $s, a \\rhd s$ is a density estimation problem.\n",
    "\n",
    "\n",
    "Pick loss function, e.g. mean-squared error, KL divergence, ...<br>\n",
    "Find parameters $\\eta$ that minimise empirical loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Examples of Models\n",
    "\n",
    "* Table Lookup Model\n",
    "\n",
    "* Linear Expectation Model\n",
    "\n",
    "* Linear Gaussian Model\n",
    "\n",
    "* Gaussian Process Model\n",
    "\n",
    "* Deep Belief Network Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Table Lookup Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Model is an explicit MDP, $\\hat{P}, \\hat{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Count visits $N(s, a)$ to each state action pair\n",
    "  * #### $$ \\hat{P}^a_{s,s'} = \\frac{1}{N(s, a)} \\sum^{T}_{t=1}{\\textbf{1}(S_t, A_t, St+1 = s, a,s')}$$\n",
    "  \n",
    "  * #### $$ \\hat{R}^a_{s} = \\frac{1}{N(s, a)} \\sum^T_{t=1}{\\textbf{1}(S_t , A_t = s, a)R_t} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  \n",
    "* Alternatively, \n",
    "  * At each time-step $t$, record experience tuple <br>\n",
    "    $\\{S_t,A_t, R_{t+1}, S_{t+1}\\}$\n",
    "  * To sample model, randomly pick tuple matching $\\{s,a,.,.\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Planning with a Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Given a model $M_\\eta = \\{P_\\eta, R_\\eta\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "* Solve the MDP $\\{S, A,P_\\eta, R_\\eta \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Using favourite planning algorithm\n",
    "  * Value iteration\n",
    "  * Policy iteration \n",
    "  * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sample-Based Planning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* A simple but powerful approach to planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Use the model only to generate samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Sample experience from model\n",
    "  $$ S_{t+1} ∼ P_\\eta(S_{t+1} | S_t , A_t)$$\n",
    "  $$ R_{t+1} = R_\\eta(R_{t+1} | S_t , A_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Apply model-free RL to samples, e.g.:\n",
    "  * Monte-Carlo control\n",
    "  * Sarsa\n",
    "  * Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Sample-based planning methods are often more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Disadvantages:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* We mostly have an imperfect model $\\{P_\\eta, R_\\eta\\} \\approx \\{P, R\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Performance of model-based RL is limited to optimal policy for approximate MDP$\\{S, A,P_\\eta, R_\\eta \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* i.e. Model-based RL is only as good as the estimated model When the model is inaccurate, planning process will compute a suboptimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* **Solution 1**: when model is wrong, use model-free RL\n",
    "* **Solution 2**: reason explicitly about model uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Dyna: Integrating Planning, Acting, and Learning</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Real and Simulated Experience\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* We consider two sources of experience\n",
    "  * Real experience Sampled from environment (true MDP)\n",
    "    $$ S' ~ P^a_{s,s'}$$\n",
    "    $$ R = R^a_s $$\n",
    "  * Simulated experience Sampled from model (approximate MDP)\n",
    "    $$ S' ~ P_\\eta(S'|S,A) $$\n",
    "    $$ R' = R_\\eta(S'|S,A) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "    \n",
    "* Real Experience has two roles:\n",
    "  * Model Learning.\n",
    "  * Direct Reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dyna\n",
    "\n",
    "<center><img src=\"img/dyna.jpeg\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tabular Dyna-Q Algorithm\n",
    "\n",
    "<center><img src=\"img/dyna_eq.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 800px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Results\n",
    "\n",
    "<center><img src=\"img/dyna_perf.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Results\n",
    "<center><img src=\"img/dyna_perf2.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## When the model is wrong\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* In  previous section, the model started out empty, and was then filled only with exactly correct information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* In general, Models may be incorrect because \n",
    "  * The environment is stochastic.\n",
    "  * Only a limited number of samples have been observed.\n",
    "  * The model was learned using function approximation that has generalized imperfectly.\n",
    "  * The environment has changed and its new behavior has not yet been observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Examples\n",
    "\n",
    "<center><img src=\"img/dynaenvchange.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Examples\n",
    "\n",
    "<center><img src=\"img/dynaenvchange2.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dyna +\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Agent keeps track for each state–action pair how many time steps elapsed since last tried in a real interaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Greater the time elapsed, the greater (we might presume) the chance that the dynamics of this pair has changed and that the model of it is incorrect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* To encourage behavior that tests long-untried actions, \n",
    "  * a special “bonus reward” is given on simulated experiences involving these actions. \n",
    "  * if the modeled reward $r$, and $\\{S,A\\}$ has not been tried in $\\tau$ time steps, <br>\n",
    "    $r =  r + κ\\sqrt{\\tau}$ , for some small κ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prioritized Sweeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Introuction\n",
    "\n",
    "* If simulated transitions are generated uniformly, then many wasteful backups will be made before stumbling onto one of these useful ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* In the much larger problems that are our real objective, the number of states is so large that an unfocused search would be extremely inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* In general, we want to work back from any state whose value has changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 'Backward focusing' of planning computations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* the agent discovers a change in the environment and changes its estimated value of one state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* The only useful one-step backups are those of actions that lead directly into the one state whose value has been changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Hence we simulate those steps first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* The frontier of useful backups propagates backward, it often grows rapidly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Hence we prioritize the backups according to a measure of their urgency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* **This is the idea behind prioritized sweeping**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Prioritized sweeping for a deterministic environment Algorithm\n",
    "\n",
    "<center><img src=\"img/psweep.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 800px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Results\n",
    "\n",
    "<center><img src=\"img/psweep_ex.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Rod Maneuvering Task\n",
    "\n",
    "* 14,400 possible states(36 angles, 20x20 Space)\n",
    "* Actions: AlongRod,PerpRod,RotateL,RotateR\n",
    "* Ackwardly placed immovable obstacles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"img/rod.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Planning as Part of Action Selection</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Introduction\n",
    "\n",
    "* The planning seen till now:\n",
    "  * The gradual improvement of a policy/value function that is good in all states generally,\n",
    "  * Not focused on any particular state.\n",
    "  * Eg: Dyna, DP.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* A different outlook:\n",
    "  * planning is begun and completed after encountering each new state $S_t$.\n",
    "  * output is not really a policy,rather a single decision, the action $A_t$.\n",
    "  * next step the planning begins anew with $S_{t+1}$ to produce $A_{t+1}$, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Some general Features of PPAS\n",
    "* the values and policy are specific to the current state and its choices,\n",
    "* They are typically discarded after being used to select the current action.\n",
    "* useful in applications in which fast responses are **not required**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Simulation-Based Search\n",
    "\n",
    "* Forward search paradigm using sample-based planning\n",
    "* Simulate episodes of experience from current time step with the model\n",
    "* Apply model-free RL to simulated episodes\n",
    "\n",
    "<center><img src=\"img/simsearch.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Simulate episodes of experience from 'now' with the model\n",
    "### $$ \\{ s_t^k,A^k_t, R^k_{t+1},... S^k_T \\} ^K_{k=1} \\sim M_\\nu $$\n",
    "\n",
    "* Apply model-free RL to simulated episodes\n",
    "  * Monte-Carlo control → Monte-Carlo search\n",
    "  * Sarsa → TD search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Simple Monte Carlo Search\n",
    "\n",
    "<center><img src=\"img/simmontesearch.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* With a perfect model and an imperfect action-value function, then deeper search will usually yield better policies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center>Monte Carlo Tree Search</center>\n",
    "\n",
    "* One of the recent and successful developments in planning.\n",
    "* Broken into two steps:\n",
    "  * Evaluation\n",
    "  * Simulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "<center><img src=\"img/mctssearch1.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Simulation\n",
    "\n",
    "<center><img src=\"img/mctssearch2.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### breakdown of MCTS\n",
    "\n",
    "<center><img src=\"img/mctssteps.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Steps:\n",
    "\n",
    "1) **Selection**: A node in the current tree is selected by some informed means as the most promising node from which to explore further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "2) **Expansion**: The tree is expanded from the selected node by adding one or more nodes as its children. \n",
    "   * Each new child node is now a leaf node of the partial game tree, meaning\n",
    "   * None of its possible moves have been visited yet in constructing the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### breakdown of MCTS\n",
    "\n",
    "<center><img src=\"img/mctssteps.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "3) **Simulation**: From one of these leaf nodes, or another node having an unvisited move,a move is selected as the start of a simulation, or rollout, of a complete game.\n",
    "   * Moves are selected by a rollout policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "4) **Backpropagation**: The result of the simulated game is backed up to update statistics attached to the links in the partial game tree traversed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "<center><img src=\"img/mcts0.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/mcts1.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/mcts2.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/mcts3.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/mcts4.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/mcts5.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Advantages of MCTS\n",
    "\n",
    "* Highly selective best-first search\n",
    "* Evaluates states dynamically (unlike e.g. DP)\n",
    "* Uses sampling to break curse of dimensionality\n",
    "* Works for “black-box” models (only requires samples)\n",
    "* Computationally efficient, anytime, parallelisable"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

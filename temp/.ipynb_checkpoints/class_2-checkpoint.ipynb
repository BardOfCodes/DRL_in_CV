{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Deep Reinforcement Learning</center>\n",
    "\n",
    "## <center>Part - II</center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## <center>Instructor: Professor R. Venkatesh Babu</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Slide Credits:\n",
    "\n",
    "1) Sutton & Barto Book: Reinforcement Learning: An Introduction: [Link To Book](http://incompleteideas.net/book/bookdraft2017nov5.pdf)\n",
    "\n",
    "2) David Silver: Introduction to Reinforcement Learning: [Link to Video Lectures](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Table Of Content\n",
    "\n",
    "1) Reintroduction\n",
    "\n",
    "2) Deep Q Learning\n",
    "\n",
    "4) Policy Gradient method\n",
    "\n",
    "5) Introduction to advanced methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Introduction</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## <center>\"Reinforcement learning is the problem of getting an agent to act in the world so as to maximize its rewards.\"</center>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<center><img src=\"img/multiarmedbandit.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 400px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>The Agent Environment Interface</center>\n",
    "\n",
    "\n",
    "<center> <img src=\"img/agent_env.PNG\" alt=\"MarkovProperty Definition\" style=\"width:700px;\"/> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Major Components of a RL Agent\n",
    "An RL agent may include one or more of these components:\n",
    "* **Policy**: agent’s behaviour function\n",
    "* **Value function**: how good is each state and/or action\n",
    "* **Model**: agent’s representation of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Maze Example\n",
    "<center><img src=\"img/e1.png\" alt=\"HistoryandState\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Maze Example: Policy\n",
    "<center><img src=\"img/e2.png\" alt=\"HistoryandState\" style=\"width: 1000px;\"/></center>\n",
    "\n",
    "* Arrows represent policy $\\pi(s)$ for each state s. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Maze Example: Value Function\n",
    "<center><img src=\"img/e3.png\" alt=\"HistoryandState\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Maze Example: Model\n",
    "<center><img src=\"img/e4.png\" alt=\"HistoryandState\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Markov Decision Process</center>\n",
    "A Markov decision process (MDP) is a Markov reward process with decisions. It is an environment in which all states are Markov.\n",
    "\n",
    "<center><img src=\"img/4.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Value Function in MDP notation\n",
    "<center><img src=\"img/7 - Copy.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Optimality Equation for $V^{*}$\n",
    "<center><img src=\"img/op3.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center> How to Learn $Q^{*}$ or $V^{*}$ ? </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Monte Carlo Backup\n",
    "\n",
    "<center><img src=\"img/MCback.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DP backup\n",
    "\n",
    "<center><img src=\"img/DPback.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TD Backup\n",
    "\n",
    "<center><img src=\"img/TDback.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center>Q-learning: Off-Policy TD Control</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "$$\n",
    "Q(S_t, A_t) = Q(S_t, A_t) + \\alpha \\big[ R_{t+1} + \\gamma {max}_a Q(S_{t+1}, a) − Q(S_t, A_t) \\big] \n",
    "$$\n",
    "\n",
    "** The learned action-value function, $Q$, directly approximates $q∗$, the optimal action-value function, independent of the policy being followed**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Q-learning: An off-policy TD control algorithm\n",
    "\n",
    "**Initialize** $Q(s, a), \\forall s \\in S, a \\in A(s)$, arbitrarily, and $Q($terminal-state$, ·) = 0$<br>\n",
    "**Repeat** (for each episode):<br>\n",
    "$\\quad$Initialize $S$<br>\n",
    "$\\quad$**Repeat** (for each step of episode):<br>\n",
    "$\\quad \\quad$ Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\\epsilon$-greedy)<br>\n",
    "$\\quad \\quad$ Take action $A$, observe $R, S'$<br>\n",
    "$\\quad \\quad$ $Q(S, A) = Q(S, A) + \\alpha \\big[ R +\\gamma {max}_a Q(S', a) − Q(S, A) \\big] $ <br>\n",
    "$\\quad \\quad$ $S = S';$<br>\n",
    "$\\quad$ until $S$ is terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>A Simple example of Tabular Q-learning</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Credits to Arthur Juliani [link](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)\n",
    "\n",
    "\n",
    "<center><img src=\"img/frozen_1.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 500px;\"/></center>\n",
    "\n",
    "* A Frozen Lake with Holes.\n",
    "* Avoid Holes, and reach the save zone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"img/frozen_2.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* we have 16 possible states (one for each block)\n",
    "* 4 possible actions (the four directions of movement)\n",
    "* giving us a 16x4 table of Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 2000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "#jList = []\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Value Function Approximation</center>\n",
    "## <center>Using Deep Learning</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Reinforcement learning can be used to solve large problems, e.g.\n",
    "<br><br>\n",
    "* Backgammon: $10^{20}$ states\n",
    "<br><br>\n",
    "* Computer Go: $10^{170}$ states\n",
    "<br><br>\n",
    "* Helicopter: continuous state space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How can we use the methods learnt previously on such huge state-spaces?\n",
    "<br><br><br>\n",
    "For eg: A Tabular methods using only Value Function of states (In half Precision) for the game of Backgammon:\n",
    "\n",
    "* Memory Requirement: $\\frac{10^{20} \\times 2}{10^{15}} = $2 Lac PetaBytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Story so far...\n",
    "\n",
    "* So far we have represented value function by a lookup table;\n",
    "<br><br>\n",
    "* Every state $s$ has an entry $V(s)$<br>\n",
    "  Or every state-action pair $s,a$ has an entry $Q(s, a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Problem with large MDPs:\n",
    "  * There are too many states and/or actions to store in memory\n",
    "  * It is too slow to learn the value of each state individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Solution for large MDPs**:\n",
    "  * Estimate value function with function approximation<br>\n",
    "### $$\\hat{v}(s, w) \\sim v_\\pi(s)$$ <br>\n",
    "    or<br>\n",
    "### $$\\hat{q}(s, a, w) \\sim q_\\pi(s, a)$$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Generalise from seen states to unseen states;<br>\n",
    "\n",
    "* Update parameter w using MC or TD learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Value Approximation\n",
    "### Types of Value Function Approximation\n",
    "\n",
    "<center><img src=\"img/func_approx.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  The Prediction Objective (MSVE)\n",
    "* Mean Squared Value Error, or MSVE:\n",
    "### $$ MSVE(\\theta) = \\sum_{s \\in S}^{}{d(s) \\big[ v_\\pi (s)-\\hat(v)(s,\\theta) \\big]^2}$$\n",
    "\n",
    "Weighting or distribution  d(s)≥0d(s)≥0  representing how much we care about the error in each state  ss .\n",
    "\n",
    "* **More states than weights**\n",
    "  * making one state’s estimate more accurate leads to making others’ less accurate. \n",
    "  * We must specify which states we care most about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Stochastic Gradient Descent for MSVE</center>\n",
    "<center><img src=\"img/fa_slides2.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Feature Vector</center>\n",
    "\n",
    "<center><img src=\"img/fa_slides3.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Linear Value Function Approximation</center>\n",
    "\n",
    "<center><img src=\"img/fa_slides4.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Incremental Prediction Methods</center>\n",
    "\n",
    "<center><img src=\"img/fa_slides6.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>TD Learning with Value Function Approximation</center>\n",
    "\n",
    "<center><img src=\"img/fa_slides8.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Control with Value Function Approximation</center>\n",
    "\n",
    "<center><img src=\"img/fa_slides10.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>A Simple example of approximate Q-learning</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Credits to Arthur Juliani [link](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)\n",
    "\n",
    "\n",
    "<center><img src=\"img/frozen_1.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 500px;\"/></center>\n",
    "\n",
    "* A Frozen Lake with Holes.\n",
    "* Avoid Holes, and reach the save zone\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"img/frozen_2.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* we have 16 possible states (one for each block)\n",
    "* 4 possible actions (the four directions of movement)\n",
    "* giving us a 16x4 table of Q-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#These lines establish the feed-forward part of the network used to choose actions\n",
    "inputs1 = tf.placeholder(shape=[1,16],dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([16,4],0,0.01))\n",
    "Qout = tf.matmul(inputs1,W)\n",
    "predict = tf.argmax(Qout,1)\n",
    "\n",
    "#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "nextQ = tf.placeholder(shape=[1,4],dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    j = 0\n",
    "    #The Q-Network\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "        a,allQ = sess.run([predict,Qout],feed_dict={inputs1:np.identity(16)[s:s+1]})\n",
    "        if np.random.rand(1) < e:\n",
    "            a[0] = env.action_space.sample()\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a[0])\n",
    "        #Obtain the Q' values by feeding the new state through our network\n",
    "        Q1 = sess.run(Qout,feed_dict={inputs1:np.identity(16)[s1:s1+1]})\n",
    "        #Obtain maxQ' and set our target value for chosen action.\n",
    "        maxQ1 = np.max(Q1)\n",
    "        targetQ = allQ\n",
    "        targetQ[0,a[0]] = r + y*maxQ1\n",
    "        #Train our network using target and predicted Q values\n",
    "        _,W1 = sess.run([updateModel,W],feed_dict={inputs1:np.identity(16)[s:s+1],nextQ:targetQ})\n",
    "        rAll += r\n",
    "        s = s1\n",
    "    jList.append(j)\n",
    "    rList.append(rAll)\n",
    "print \"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center>DQN in ATARI</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The model\n",
    "\n",
    "<center><img src=\"img/fa2_ex1.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Performance\n",
    "\n",
    "<center><img src=\"img/fa2_ex2.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Benefits of Experience Replay and Double DQN\n",
    "\n",
    "<center><img src=\"img/fa2_ex3.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center> Deep Q-Learning in Computer Vision</center>\n",
    "\n",
    "### Action-Decision Networks for Visual Tracking with Deep Reinforcement Learning\n",
    "[Link](http://openaccess.thecvf.com/content_cvpr_2017/papers/Yun_Action-Decision_Networks_for_CVPR_2017_paper.pdf)\n",
    "\n",
    "Video : [https://www.youtube.com/watch?v=q8HU_bK8LOk](https://www.youtube.com/watch?v=q8HU_bK8LOk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Policy Gradients</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Introduction </center>\n",
    "<center><img src=\"img/pg_1.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Why Policy-Based RL</center>\n",
    "<center><img src=\"img/pg_3.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Can Learning Policy be easier than Learning Values of states?</center>\n",
    "* The policy may be a simpler function to approximate.\n",
    "* This is the simplest advantage that policy parameterization may have over action-value parameterization.\n",
    "\n",
    "Why?\n",
    "* Problems vary in the complexity of their policies and action-value functions. \n",
    "* For some, the action-value function is simpler and thus easier to approximate. \n",
    "* For others, the policy is simpler. \n",
    "\n",
    "\n",
    "** In the latter case a policy-based method will typically be faster to learn and yield a superior asymptotic policy.**\n",
    "\n",
    "Example: In Robotics Tasks with continuous Action space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Example of Stochastic Optimal Policy</center>\n",
    "<center><img src=\"img/pg_4.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>REINFORCE: Simplest Policy Gradient Method</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Quality Measure of Policy</center>\n",
    "<center><img src=\"img/pg_8.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Analytic Gradient Ascent</center>\n",
    "<center><img src=\"img/pg_12.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Example- Softmax Policy</center>\n",
    "<center><img src=\"img/pg_13.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Example- Gaussian Policy</center>\n",
    "<center><img src=\"img/pg_14.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>One-step MDP</center>\n",
    "<center><img src=\"img/pg_15.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Monte-Carlo Policy Gradient (REINFORCE)</center>\n",
    "<center><img src=\"img/pg_17.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Policy Gradient Example</center>\n",
    "\n",
    "[https://www.youtube.com/watch?v=m-DiH_Fq6lg](https://www.youtube.com/watch?v=m-DiH_Fq6lg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center>Policy Gradient in Computer Vision</center>\n",
    "\n",
    "### Visual Tracking by Reinforced Decision Making\n",
    "[Link](https://arxiv.org/pdf/1702.06291.pdf)\n",
    "\n",
    "\n",
    "<center><img src=\"img/pg_ex_2.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Actor Critic Methods</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Value-Based Vs Policy-Based RL</center>\n",
    "<center><img src=\"img/pg_2.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Reducing Variance Using a Critic</center>\n",
    "<center><img src=\"img/pg_19.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Estimating the Action-Value Function</center>\n",
    "<center><img src=\"img/pg_20.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Action Value Actor Critic</center>\n",
    "<center><img src=\"img/pg_21.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>The Path Ahead</center>\n",
    "\n",
    "## The Field Explodes from a singular narrative from here:\n",
    "  \n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "  * Additional Enhacements for reducing variance : Experience Replay, AC with baseline etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "  * More Complex architecture: Double DQN, A3C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "  * Other Optimization Routes: TRPO, TRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "  * Other kinds of RL: Inverse RL, Imitation Learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center>Summary</center>\n",
    "\n",
    "1) Deep Q - learning\n",
    "\n",
    "2) Policy Gradient\n",
    "\n",
    "3) Actor Critic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center>Any Questions?</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Reminder: Quiz on 26th March"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

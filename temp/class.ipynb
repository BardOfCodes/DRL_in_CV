{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Deep Reinforcement Learning</center>\n",
    "\n",
    "## <center>Instructor: Professor R. Venkatesh Babu</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Table Of Content\n",
    "\n",
    "1) Introduction and Applications\n",
    "\n",
    "2) Formal Definition and Mathematical Tools (MDPs)\n",
    "\n",
    "3) How to Implement RL?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Some Recent Achievements with Reinforcement Learning </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RL for Games\n",
    "\n",
    "* AI for ATARI 2600 Games: 2014 [Link](https://arxiv.org/pdf/1312.5602v1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* AI for the game of Go: 2016 [Link](https://www.nature.com/articles/nature16961)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* AI for the game of DOTA: 2017 [Link](https://blog.openai.com/dota-2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Alpha Go Zero, Learning from scratch: 2017 [Link](https://www.nature.com/articles/nature24270)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* AI for Poker, Libertus: 2017 [Link](https://www.ijcai.org/proceedings/2017/0772.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* RL for 3D Video Games Review: 2017 [Link](https://arxiv.org/pdf/1708.07902.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Robotics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Multiple Visio-motor task in Robotics: 2016 [Link](https://arxiv.org/abs/1504.00702)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Learning to Navigate in Complex Environments: 2017 [Link](https://arxiv.org/abs/1611.03673)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "* Dialogue Systems, Machine Translations, Text generation : [Link](https://arxiv.org/pdf/1701.07274.pdf)\n",
    "\n",
    "## Operations Research \n",
    "\n",
    "* Business Management, Finance, Healthcare, transportation:  [Link](https://arxiv.org/pdf/1701.07274.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Computer Vision\n",
    "\n",
    "* **2016** \n",
    "  * Reinforcement Learning for Visual Object Detection\n",
    "  * Active Object Localization With Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **2017**\n",
    "  * A2-RL: Aesthetics Aware Reinforcement Learning for Image Cropping\n",
    "  * Tracking as Online Decision-Making: Learning a Policy From Streaming Videos With Reinforcement Learning\n",
    "  * Learning Cooperative Visual Dialog Agents With Deep Reinforcement Learning\n",
    "  * Attention-Aware Deep Reinforcement Learning for Video Face Recognition\n",
    "  * 3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-Scale 3D Point Clouds\n",
    "  * Deep Reinforcement Learning-Based Image Captioning With Embedding Reward\n",
    "  * Attention-Aware Face Hallucination via Deep Reinforcement Learning\n",
    "  * Deep Variation-Structured Reinforcement Learning for Visual Relationship and Attribute Detection\n",
    "  * Collaborative Deep Reinforcement Learning for Joint Object Search\n",
    "  * Action-Decision Networks for Visual Tracking With Deep Reinforcement Learning\n",
    "  * PoseAgent: Budget-Constrained 6D Object Pose Estimation via Reinforcement Learning\n",
    "  * A Reinforcement Learning Approach to the View Planning Problem\n",
    "  * A Joint Speaker-Listener-Reinforcer Model for Referring Expressions\n",
    "  * Ask the Right Questions: Active Question Reformulation with Reinforcement Learning\n",
    "  * Deep Reinforcement Learning for Unsupervised Video Summarization with Diversity-Representativeness Reward\n",
    "  * Deep Reinforcement Learning for Image Hashing\n",
    "  * Modeling Attention in Panoramic Video: A Deep Reinforcement Learning Approach\n",
    "  * Video Captioning via Hierarchical Reinforcement Learning\n",
    "  * Accelerated Methods for Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> 19 papers in ICLR18,24 papers in NIPS17! </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>RL Formal Definition</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\"Reinforcement learning is the problem of getting an agent to act in the world so as to maximize its rewards.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " For example, consider teaching a dog a new trick: you cannot tell it what to do, but you can reward/punish it if it does the right/wrong thing. It has to figure out what it did that made it get the reward/punishment, which is known as the credit assignment problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center>RL vs Supervised Learning vs Unsupervised Learning</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RL vs Supervised Learning\n",
    "* Training Examples\n",
    "    * Supervised Learning: Training examples from a knowledgeable external supervisor (situation together with a label).\n",
    "    * RL: No such training examples.\n",
    "* Objective Functions\n",
    "    * Supervised Learning: Aim is to extrapolate, or generalize so that it acts correctly in situations not present in the training set. \n",
    "    * In RL, it is often impractical to obtain examples of desired behavior that are both correct and representative of all the situations and an agent must be able to learn from its own experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### RL vs Unsupervised Learning\n",
    "* Unsupervised Learning is about finding structure hidden in collections of unlabeled data.\n",
    "* Uncovering structure in an agent’s experience can certainly be useful in reinforcement learning, but by itself does not address the reinforcement learning agent’s problem of maximizing a reward signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Important RL Terms and Definitions</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Goal of RL (Reward Hypothesis)\n",
    "<center><img src=\"img/3.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Interaction between Agent and Environment\n",
    "<center><img src=\"img/7.png\" alt=\"RewardHypothesis\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## History and State\n",
    "<center><img src=\"img/8.png\" alt=\"HistoryandState\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Information State\n",
    "<center><img src=\"img/11.png\" alt=\"HistoryandState\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Major Components of a RL Agent\n",
    "An RL agent may include one or more of these components:\n",
    "* **Policy**: agent’s behaviour function\n",
    "* **Value function**: how good is each state and/or action\n",
    "* **Model**: agent’s representation of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Policy\n",
    "* A policy is the agent’s behaviour\n",
    "* It is a map from state to action, e.g.\n",
    "* Deterministic policy: $a = π(s)$\n",
    "* Stochastic policy: $π(a|s) = P[A_t = a|S_t = s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Value function\n",
    "* Value function is a prediction of future reward\n",
    "* Used to evaluate the goodness/badness of states\n",
    "* And therefore to select between actions, e.g.\n",
    "$$v_π(s) = E_π[R _{t+1} + γ*R_{t+2} + γ^{2}*R_{t+3} + ... | S_t = s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model\n",
    "* A model predicts what the environment will do next\n",
    "* **Transitions**: P predicts the next state (i.e. dynamics)\n",
    "* **Rewards**: R predicts the next (immediate) reward, e.g.\n",
    "$$ P_{ss'} = P[S_{t+1} = s' | S_t = s, A_t = a]$$\n",
    "$$ R^{a}_{s} = E[R_{t+1} | S_t = s, A_t = a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maze Example\n",
    "<center><img src=\"img/e1.png\" alt=\"HistoryandState\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Maze Example: Policy\n",
    "<center><img src=\"img/e2.png\" alt=\"HistoryandState\" style=\"width: 1000px;\"/></center>\n",
    "\n",
    "* Arrows represent policy $\\pi(s)$ for each state s. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Maze Example: Value Function\n",
    "<center><img src=\"img/e3.png\" alt=\"HistoryandState\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Maze Example: Model\n",
    "<center><img src=\"img/e4.png\" alt=\"HistoryandState\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Building Blocks of MDP</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## <center>The Agent Environment Interface</center>\n",
    "\n",
    "\n",
    "<center> <img src=\"img/agent_env.PNG\" alt=\"MarkovProperty Definition\" style=\"width:700px;\"/> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Markov Decision Process</center>\n",
    "A Markov decision process (MDP) is a Markov reward process with decisions. It is an environment in which all states are Markov.\n",
    "\n",
    "<center><img src=\"img/4.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Policy in MDP notation\n",
    "<center><img src=\"img/6.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>\n",
    "* A policy fully defines the behaviour of an agent\n",
    "* MDP policies depend on the current state (not the history)\n",
    "* i.e. Policies are **stationary** (time-independent),\n",
    "    $A_t ∼ π(·|S_t ), \\forall t > 0$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Return\n",
    "<center><img src=\"img/3 - Copy.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>\n",
    "* The discount $γ ∈ [0, 1]$ is the present value of future rewards\n",
    "* The value of receiving reward R after k + 1 time-steps is $γ^k R$.\n",
    "* This values immediate reward above delayed reward.\n",
    "    * $γ$ close to 0 leads to ”myopic” evaluation\n",
    "    * $γ$ close to 1 leads to ”far-sighted” evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Value Function in MDP notation\n",
    "<center><img src=\"img/7 - Copy.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Bellman Optimality Equation</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimal Value Function\n",
    "<center><img src=\"img/o.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimal Policy\n",
    "<center><img src=\"img/o1.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Optimality Equation for $V^{*}$\n",
    "<center><img src=\"img/op3.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Optimality Equation for $Q^{*}$\n",
    "<center><img src=\"img/op4.png\" alt=\"Matrix\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> How to Learn $Q^{*}$ or $V^{*}$ ? </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Monte Carlo Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Roughly speaking, Monte Carlo methods wait until the return following the visit is known, then use that return as a target for $V(S_t)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "#### Constant $\\alpha$ MC\n",
    "\n",
    "$$\n",
    "V(S_t) = V(S_t) + \\alpha\\big[ G_t - V(S_t) \\big]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* $G_t$ is the actual return following time t,\n",
    "* $\\alpha$ is a constant step-size parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Monte Carlo Backup\n",
    "\n",
    "<center><img src=\"img/MCback.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DP methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Although DP methods update at every iteration, they use a model of the environment for the update. Basically they use, $p(s',r|s,a)$, the transition probability values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "#### Iterative policy evaluation in DP\n",
    "$$\n",
    "V (s) = \\sum_{a}\\pi(a|s) \\sum_{s',r}{p(s',r|s,a)\\big[ r+\\gamma V(s') \\big]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DP backup\n",
    "\n",
    "<center><img src=\"img/DPback.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Temporal Difference Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "TD methods need wait only until the next time step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "#### TD(0)\n",
    "\n",
    "$$\n",
    "V(S_t) = V(S_t) + \\alpha\\big[ R_{t+1} +\\gamma V(S_{t+1})- V(S_t) \\big]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* target for the Monte Carlo update is $G_t$, whereas the target for the TD update is $R_{t+1} + \\gamma V(S_{t+1})$.\n",
    "* Because the TD method bases its update in part on an existing estimate, we say that it is a bootstrapping method, like DP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TD Backup\n",
    "\n",
    "<center><img src=\"img/TDback.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Monte Carlo Backup\n",
    "\n",
    "<center><img src=\"img/MCback.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 200px;\"/></center>\n",
    "\n",
    "#### DP backup\n",
    "\n",
    "<center><img src=\"img/DPback.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 200px;\"/></center>\n",
    "\n",
    "#### TD Backup\n",
    "\n",
    "<center><img src=\"img/TDback.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 200px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"img/bootsam.JPG\" alt=\"Multi-armed Bandit\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center>Q-learning: Off-Policy TD Control</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$$\n",
    "Q(S_t, A_t) = Q(S_t, A_t) + \\alpha \\big[ R_{t+1} + \\gamma {max}_a Q(S_{t+1}, a) − Q(S_t, A_t) \\big] \n",
    "$$\n",
    "\n",
    "** The learned action-value function, $Q$, directly approximates $q∗$, the optimal action-value function, independent of the policy being followed**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Q-learning: An off-policy TD control algorithm\n",
    "\n",
    "**Initialize** $Q(s, a), \\forall s \\in S, a \\in A(s)$, arbitrarily, and $Q($terminal-state$, ·) = 0$<br>\n",
    "**Repeat** (for each episode):<br>\n",
    "$\\quad$Initialize $S$<br>\n",
    "$\\quad$**Repeat** (for each step of episode):<br>\n",
    "$\\quad \\quad$ Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\\epsilon$-greedy)<br>\n",
    "$\\quad \\quad$ Take action $A$, observe $R, S'$<br>\n",
    "$\\quad \\quad$ $Q(S, A) = Q(S, A) + \\alpha \\big[ R +\\gamma {max}_a Q(S', a) − Q(S, A) \\big] $ <br>\n",
    "$\\quad \\quad$ $S = S';$<br>\n",
    "$\\quad$ until $S$ is terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "1) Introduction to RL and its Applications\n",
    "\n",
    "2) The Intuitive + formal Formmulation of mathematical tools, like MDPs\n",
    "\n",
    "3) A brief overview of methods to implement RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Next Session\n",
    "\n",
    "1) Deep Q-learning\n",
    "\n",
    "2) Policy Gradient\n",
    "\n",
    "3) Latest Methods: TRPO,PPO,AlphaGoZero Review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Any Questions? </center>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

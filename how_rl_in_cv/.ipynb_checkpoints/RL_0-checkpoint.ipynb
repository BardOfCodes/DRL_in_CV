{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Reinforcement Learning in Computer Vision</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We will look at three Computer Vision tasks, namely-\n",
    "    \n",
    "    * Object Detection\n",
    "    * Action Detection\n",
    "    * Visual Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* For each task we will try answering these questions -\n",
    "    * What is the task?\n",
    "    * Can we identify the RL components -\n",
    "        * The MDP formulation\n",
    "            * State Space\n",
    "            * Action Space\n",
    "            * Reward System\n",
    "        * Network Architecture\n",
    "    * Why use RL for this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Task 1: Object Detection </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/1_1.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><img src=\"img/1.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is the task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/5.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some Important KeyPoints -\n",
    "* This is one of the fundamental Computer Vision Tasks\n",
    "* This has been viewed as a Supervised Learning so far\n",
    "* Some of the popular Approaches -\n",
    "    * Selective Search\n",
    "    * CPMC\n",
    "    * Edge Boxes (based on sliding windows)\n",
    "    * R-CNN\n",
    "    * Fast R-CNN\n",
    "    * Faster R-CNN\n",
    "    * YoLo\n",
    "    * ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What's the idea here?\n",
    "\n",
    "* A class-specific active detection model that learns to **localize target objects** known by the system. \n",
    "* Model follows a **top-down search strategy**, which starts by analyzing the whole scene and then proceeds to narrow down the correct location of objects.\n",
    "* Achieved by applying a **sequence of transformations** to a box that initially covers a large region of the image and is finally reduced to a tight bounding box.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example Output\n",
    "<center><img src=\"img/1_2.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to look it as a RL problem?\n",
    "\n",
    "**Question 1**: How to think of this as a **Sequential Decision Making** Problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer 1**: At each time step, the agent should decide in which region of the image to focus its attention so that it can find objects in a few steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Question 2**: How to cast this as a **Markov Decision Process**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer 2**: We cast the problem of object localization as a Markov decision process (MDP) since this setting provides a formal framework to model an agent that makes a sequence of decisions. We will try to identify the components of MDP, the set of actions A, a set of states S, and a reward function R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Identifying MDP Parameters\n",
    "\n",
    "### Action Space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The **set of actions A** is composed of **eight transformations** that can be applied to the box and one action to terminate the search process.\n",
    "<center><img src=\"img/1_3.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### State Space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* State Repreentation = tuple(o,h)\n",
    "* o = feature vector of the observed region \n",
    "    * ((fc6) output => 4,096 dimensional feature vector to represent its content)\n",
    "* h = vector with the history of taken actions\n",
    "    * The history vector encodes 10 past actions\n",
    "    * Actions encoded as a 9-dimensional binary vector\n",
    "    \n",
    "Motivation behind the history vector: Useful to stabilize search trajectories that might get stuck in repetitive cycles, improving average precision\n",
    "by approximately 3 percent points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Rewards?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Motivation:\n",
    "\n",
    "* Reward function R is proportional to the improvement that the agent makes to localize an object after selecting a particular action\n",
    "* Measured using the Intersection-over-Union (IoU) between the target object and the predicted box at any given time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/1_10.png\" alt=\"Example1\"/></center>\n",
    "<center><img src=\"img/1_11.png\" alt=\"Example1\"/></center>\n",
    "<center><img src=\"img/1_12.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "where,\n",
    "b = be the box of an observable region, and \n",
    "\n",
    "g = the ground truth box for a target object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Explanation:\n",
    "* Given a state s, those actions will be **rewarded** that result in a **higher IOU with the groudtruth**, otherwise the actions are penalised. \n",
    "* For trigger action, reward is positive if final IOU with groundtruth is greater than a certin threshold, and negative otherwise. \n",
    "* This **reward scheme is binary** r ∈ {−1, +1}, and applies to any action that transforms the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Network Architecture\n",
    "<center><img src=\"img/1_4.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some Quantitative Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/1_5.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some Qualitative Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/1_8.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/1_9.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why to use RL here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/1_6.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/1_7.png\" alt=\"Example1\"/></center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

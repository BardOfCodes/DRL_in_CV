{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Reinforcement Learning in Computer Vision</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We will look at three Computer Vision tasks, namely-\n",
    "    \n",
    "    * Object Detection\n",
    "    * Action Detection\n",
    "    * Visual Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* For each task we will try answering these questions -\n",
    "    * What is the task?\n",
    "    * Can we identify the RL components -\n",
    "        * The MDP formulation\n",
    "            * State Space\n",
    "            * Action Space\n",
    "            * Reward System\n",
    "        * Network Architecture\n",
    "    * Why use RL for this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Task 1: Object Detection </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/1_1.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><img src=\"img/1.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is the task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/5.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some Important KeyPoints -\n",
    "* This is one of the fundamental Computer Vision Tasks\n",
    "* This has been viewed as a Supervised Learning so far\n",
    "* Some of the popular Approaches -\n",
    "    * Selective Search\n",
    "    * CPMC\n",
    "    * Edge Boxes (based on sliding windows)\n",
    "    * R-CNN\n",
    "    * Fast R-CNN\n",
    "    * Faster R-CNN\n",
    "    * YoLo\n",
    "    * ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What's the idea here?\n",
    "\n",
    "* A class-specific active detection model that learns to **localize target objects** known by the system. \n",
    "* Model follows a **top-down search strategy**, which starts by analyzing the whole scene and then proceeds to narrow down the correct location of objects.\n",
    "* Achieved by applying a **sequence of transformations** to a box that initially covers a large region of the image and is finally reduced to a tight bounding box.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example Output\n",
    "<center><img src=\"img/1_2.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to look it as a RL problem?\n",
    "\n",
    "**Question 1**: How to think of this as a **Sequential Decision Making** Problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer 1**: At each time step, the agent should decide in which region of the image to focus its attention so that it can find objects in a few steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Question 2**: How to cast this as a **Markov Decision Process**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer 2**: We cast the problem of object localization as a Markov decision process (MDP) since this setting provides a formal framework to model an agent that makes a sequence of decisions. We will try to identify the components of MDP, the set of actions A, a set of states S, and a reward function R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Identifying MDP Parameters\n",
    "\n",
    "### Action Space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The **set of actions A** is composed of **eight transformations** that can be applied to the box and one action to terminate the search process.\n",
    "<center><img src=\"img/1_3.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### State Space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* State Representation = tuple(o,h)\n",
    "* o = feature vector of the observed region \n",
    "    * ((fc6) output => 4,096 dimensional feature vector to represent its content)\n",
    "* h = vector with the history of taken actions\n",
    "    * The history vector encodes 10 past actions\n",
    "    * Actions encoded as a 9-dimensional binary vector\n",
    "    \n",
    "Motivation behind the history vector: Useful to stabilize search trajectories that might get stuck in repetitive cycles, improving average precision\n",
    "by approximately 3 percent points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Rewards?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Motivation:\n",
    "\n",
    "* Reward function R is proportional to the improvement that the agent makes to localize an object after selecting a particular action\n",
    "* Measured using the Intersection-over-Union (IoU) between the target object and the predicted box at any given time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/1_10.png\" alt=\"Example1\"/></center>\n",
    "<center><img src=\"img/1_11.png\" alt=\"Example1\"/></center>\n",
    "<center><img src=\"img/1_12.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "where,\n",
    "b = be the box of an observable region, and \n",
    "\n",
    "g = the ground truth box for a target object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Explanation:\n",
    "* Given a state s, those actions will be **rewarded** that result in a **higher IOU with the groudtruth**, otherwise the actions are penalised. \n",
    "* For trigger action, reward is positive if final IOU with groundtruth is greater than a certin threshold, and negative otherwise. \n",
    "* This **reward scheme is binary** r ∈ {−1, +1}, and applies to any action that transforms the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Network Architecture\n",
    "<center><img src=\"img/1_4.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some Quantitative Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/1_5.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some Qualitative Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/1_8.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/1_9.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why to use RL here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/1_6.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/1_7.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Task 2: Action Detection </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/a.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is the task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/2_13.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Does an action occur?\n",
    "* When does the action occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Whats the idea here?\n",
    "\n",
    "* A class-specific action-detection model.\n",
    "* It learns to continually adjust the current region to cover th groundtruth more precisely in a self-adapted way\n",
    "* This is achieved by applying a sequence of transformations to a temporal window that is initially placed in the video at random and finally finds and covers action region as large as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example Output\n",
    "<center><img src=\"img/2_2.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to look at it as a RL problem?\n",
    "\n",
    "** Question 1:** How to cast this as a Markov Decision Process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer 1:** The problem is cast as a MDP, in which the agent interacts with the environment and makes a sequence of decisions to achieve the settled goal. \n",
    "* In our formulation, the environment is the input video clip, \n",
    "* in which the agent has an observation of the current video segment, called temporal window, \n",
    "* and through the actions he restructures the position or span of the window, to achieve the goal of locating the actions precisely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Identifying MDP Parameters\n",
    "\n",
    "### Action Space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/c.png\" alt=\"Example1\"/></center>\n",
    "All the actions are regular actions, except **jump**. \n",
    "\n",
    "The idea behind this irregular action, is to translate the temporal window to a new position away from the current site to avoid  that the agent traps itself round the present location when there is no motion occurin nearby. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### State Space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* State Repreentation = composed of two components (current temporal window, history of actions taken)\n",
    "* current temporal window => to describe the motion within the current window\n",
    "    * features extracted from C3D CNN model (pre-trained on Sports-1M)\n",
    "    * ((fc6) output => 4,096 dimensional feature vector to represent its content)\n",
    "* history of actions taken = vector with the history of taken actions\n",
    "    * The history vector encodes 10 past actions\n",
    "    * Actions encoded as a 7-dimensional binary vector\n",
    "    \n",
    "Motivation behind the history vector: Useful to stabilize search trajectories that might get stuck in repetitive cycles, improving average precision\n",
    "by approximately 3 percent points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Rewards?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Motivation:\n",
    "\n",
    "* Reward function R awards the agent for ctions that will bring about the improvement of motion localisation accuracy\n",
    "* And, the agent is penalized for actions that leads to the decline of the accuracy\n",
    "* Measured using the Intersection-over-Union (IoU) between the current attended temporal window and the groundtruth region of motion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/x.png\" alt=\"Example1\"/></center>\n",
    "where,\n",
    "w = current window.\n",
    "\n",
    "g = groundtruth region of motion\n",
    "\n",
    "<center><img src=\"img/y.png\" alt=\"Example1\"/></center>\n",
    "\n",
    "where,\n",
    "w and w' are attended windows corresponding to state s and s' respectively. \n",
    "\n",
    "n = number of groundtruths within the video\n",
    "\n",
    "<center><img src=\"img/z.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Network Architecture\n",
    "\n",
    "<center><img src=\"img/b.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some Quantitative Results\n",
    "<center><img src=\"img/e.png\" alt=\"Example1\"/></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some Qualitative Results\n",
    "<center><img src=\"img/f.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why use RL here?\n",
    "<center><img src=\"img/d.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Task 3: Visual Tracking </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/adnet_1.png\" style=\"width:700px;\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Task at hand:\n",
    "\n",
    "### Single Line:\n",
    "* Visual tracking solves the problem of finding the position of the target in a new frame from the current position.\n",
    "\n",
    "### Definition: \n",
    "\n",
    "* Visual Tracking is the process of locating, identifying, and determining the dynamic configuration of one or many moving (possibly deformable) objects (or parts of objects) in each frame of one or several cameras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tracking Challenges\n",
    "\n",
    "* **Object Modeling**: how to define what an object is in terms that can be interpreted by a computer ?\n",
    "* **Appearance Change**: The observation of an object changes according to many parameters (illumination conditions, occlusions, shape variation...)\n",
    "* **Kinematic Modelling**: How to inject priors on object kinematic and interactions between objects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Common hurdles\n",
    "* Track\tinitaition and termination\t\n",
    "* Occlusion\thandling\t\n",
    "* Merging/switching\t\n",
    "* Drifting\tdue\tto\twrong\tupdate\tof\tthe\ttarget\t model\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Existing Methods\n",
    "\n",
    "* Various Classical tracking methods, KLT Tracker etc.\n",
    "* Recent CNN-Based:\n",
    "  * Learning Multi-Domain Convolutional Neural Networks for Visual Tracking CVPR'16\n",
    "  * Siamese Instance Search for Tracking, CVPR'16\n",
    "* They still require computationally inefficient search algorithms, such as sliding window or candidate sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why RL:\n",
    "\n",
    "* No post-processing: bounding box regression\n",
    "* Fewer searching steps than sliding window or candidate sampling approaches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Focus of this paper:\n",
    "### Solution to existing works for:\n",
    "\n",
    "1) Present better algorithm than the existing inefficient search algorithms that explore the region of interest and select the best candidate by matching with the tracking model, and\n",
    "\n",
    "2) Presenting a method to train using unlabeled frames in a semi-supervised case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MDP Formulation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Visual tracking solves the problem of finding the position of the target in a new frame from the current position.\n",
    "\n",
    "* The proposed tracker dynamically pursues the target by sequential actions\n",
    "\n",
    "* The tracker is defined as an agent of whose goal is to capture the target with a bounding box shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center>The MDP Parameters:</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# State Space:\n",
    "\n",
    "* **Goal** : Capture Required Information.\n",
    "* **What useful Information must the state capture?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* State is defined by tuple ($p_t,d_t$)\n",
    "* $p_t \\in R^{112 \\times 112 \\times 3}$:  denotes the image patch within the bounding box\n",
    "* $d_t \\in R^{110}$: represents the dynamics of actions denoted by a vector containing the previous $k$ actions at $t$-th iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Action Space\n",
    "\n",
    "* In this paper: Discrete Actions\n",
    "* What can be the actions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/adnet_3.png\" style=\"width:400px;\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Transition Probabilities:\n",
    "\n",
    "* After decision of action at in state $s_t$, the next state $s_{t+1}$ is obtained by the state transition functions.\n",
    "* Will it be a stochastic output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Reward:\n",
    "At the termination step $T$, that is, $a_T$ is ‘stop’ action,$r(s_T )$ is assigned by:\n",
    "<center><img src=\"img/adnet_6.png\" style=\"width:400px;\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Network Architecture\n",
    "<center><img src=\"img/adnet_2.png\" style=\"width:700px;\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Action Selection:\n",
    "\n",
    "<center><img src=\"img/adnet_5.png\" style=\"width:1000px;\" alt=\"Example1\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "<center><img src=\"img/adnet_4.png\" style=\"width:500px;\" alt=\"Example1\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "<center><img src=\"img/adnet_8.png\" style=\"width:500px;\" alt=\"Example1\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Analysis on the actions. \n",
    "\n",
    "* most of the frames require fewer than five actions to pursue the target in each frame!\n",
    "\n",
    "<center><img src=\"img/adnet_7.png\" style=\"width:700px;\" alt=\"Example1\"/></center>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

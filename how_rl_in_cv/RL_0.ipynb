{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Reinforcement Learning in Computer Vision</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We will look at three Computer Vision tasks, namely-\n",
    "    \n",
    "    * Object Detection\n",
    "    * Action Detection\n",
    "    * Visual Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* For each task we will try answering these questions -\n",
    "    * What is the task?\n",
    "    * Can we identify the RL components -\n",
    "        * The MDP formulation\n",
    "            * State Space\n",
    "            * Action Space\n",
    "            * Reward System\n",
    "        * Network Architecture\n",
    "    * Why use RL for this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Task 1: Object Detection </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/1.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is the task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/5.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some Important KeyPoints -\n",
    "* This is one of the fundamental Computer Vision Tasks\n",
    "* This has been viewed as a Supervised Learning so far\n",
    "* Some of the popular Approaches -\n",
    "    * Selective Search\n",
    "    * CPMC\n",
    "    * Edge Boxes (based on sliding windows)\n",
    "    * R-CNN\n",
    "    * Fast R-CNN\n",
    "    * Faster R-CNN\n",
    "    * YoLo\n",
    "    * ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What's the idea here?\n",
    "\n",
    "* Perform Hierarchical Object Detection in images guided by a Deep Reinforcement Learning\n",
    "* Focus on those parts of the image that contain richer information and zoom on them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example\n",
    "<center><img src=\"img/6.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to look it as a RL problem?\n",
    "\n",
    "**Question 1**: How to think of this as a **Sequential Decision Making** Problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer 1**: At each time step, the agent should decide in which region of the image to focus its attention so that it can find objects in a few steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Question 2**: How to cast this as a **Markov Decision Process**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer 2**: The problem is cast as a MDP, that provides a framework to model decision making when outcomes are pretty uncertain. Now, we need to identify parameters of the MDP, namely States, Actions and Rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Identifying MDP Parameters\n",
    "\n",
    "#### What do the States look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**State** is composed by the descriptor of the **current region** and a **memory vector**.\n",
    "\n",
    "**Memory Vector** captures last 4 actions that the agent has already performed in the search for an object. The past 4 actions are encoded in a one-shot vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### What do the Actions look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are two types of possible actions-\n",
    "* **Movement Actions** : These imply a change in the current observed region (5 actions).\n",
    "* **Terminal Action** : To indicate that the object is found and search has ended. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/2.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/3.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### What does the Reward look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/8.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Explanation:\n",
    "* Given a state s, those actions will be rewarded that result in a higher IOU with the groudtruth, otherwise the actions are penalised. \n",
    "* For trigger action, reward is positive if final IOU with groundtruth is greater than a certin threshold, and negative otherwise. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Network Architecture\n",
    "<center><img src=\"img/4.png\" alt=\"Example1\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why use RL here?\n",
    "Claim: Objects can be detected with very few proposals from an appropriate hierarchy.\n",
    "<center><img src=\"img/7.png\" alt=\"Example1\"/></center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Reinforcement Learning in Non-Associative setting</center>\n",
    "\n",
    "#### <center> Reference: Chapter 2, Sutton and Barto</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Multi-arm Bandit Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) Action Value Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4) Improving Exploration in Simple Bandit Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Introduction</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Reinforcement Learning: Uses information that **evaluates the actions** taken rather than instructs by giving correct actions(as in supervised Learning).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Non-Associative setting: Learning to act in just one situation.\n",
    "\n",
    "* Environment is not dynamic.\n",
    "* Very simplified setting.\n",
    "* Full reinforcement learning is much more complex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Example of Associative setting: Driving, amount of time spent reading in a day.<br>\n",
    "Example of Non-Associative setting: k-Arm Bandit Problem, Animals learning not to fear stuffed animals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Talk about non associative learning in Psycology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Goal\n",
    "\n",
    "* Show how evaluative feedback differs from instructive feedback.\n",
    "* Give an introduction on how to solve Reinforcement learning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "694627ec-193b-4e7b-98f6-f6c2dfd16c47"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# <center>Multi-arm Bandit Problems</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "**k-Armed Bandit Problem: ** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Every time step you have k different options, or actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* receive a numerical reward chosen from a stationary probability distribution based on the action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* objective: maximize the expected total reward over some time period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/multiarmedbandit.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 400px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Each action has an expected/mean reward given that that action is selected given by $q_* (a) $.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$q_* (a) $ can be written as:\n",
    "\n",
    "\n",
    "$$ \\mathbf{ q_* (a) = E(R_t | A_t = a) } $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "**Notation**\n",
    "* $A_t$ = action selected at time step $t$\n",
    "* $R_t$ = corresponding reward (at time step $t$)\n",
    "* $q_* (a) $ = Value of an arbitrary action 'a' . \n",
    "* $Q_t(a)$ = Estimated  action value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " If $\\mathbf{q_* (a) }$ is known: we will select action '**a**' with highest $\\mathbf{q_* (a) }$.\n",
    " \n",
    "But we do not have $\\mathbf{q_* (a) }$.What can we do?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can find a close estimate $\\mathbf{Q_t (a) \\approx q_* (a) }$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Should we accept our estimate and derive actions?\n",
    "* Should we try to improve our estimate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exploring vs Exploiting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "At any time step: select action with highest $\\mathbf{Q_t (a)}$ ==> **Exploiting**.\n",
    "\n",
    "At any time step: select action which does not have the  highest $\\mathbf{Q_t (a)}$ ==> **Exploring**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Why explore? \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* To reduce the uncertainity of $\\mathbf{Q_t (a)}$ for all actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Through exploration, you may find an action with greater $\\mathbf{Q_t (a)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/multiarmedbandit.jpg\" alt=\"Multi-armed Bandit\" style=\"width: 400px;\"/></center>\n",
    "\n",
    "This is the k-armed bandit problem, where ** Exploration** and **exploitation** has to be balanced. \n",
    "\n",
    "**Now, we will look at how to solve it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "While it seems silimar to generalization, it is indeed more complex. The models we will eventually see, would have to generalize to unseen states, as well as explore. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# <center>Action-Value Method</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Action-Value Method\n",
    "\n",
    "* Find an estimate $Q_t(a) \\sim q_*(a)$ for each action. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "#### Sample-Average method\n",
    "\n",
    "Simplest way of estimating $Q_t(a)$, given $ Q_t (a) \\approx q_* (a) = E(R_t | A_t = a)$\n",
    "\n",
    "$$ Q_t (a) = \\frac{\\sum^{t-1}_{i=1}{R_i . 1_{A_i = a}}}{\\sum^{t-1}_{i=1}{1_{A_i = a}}} $$\n",
    "\n",
    "If the denominator is zero, then we instead define $Q_t(a)$ as some default value, such as $Q_1(a) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now that we have the estimate $Q_t(a) \\forall a \\in A$, we need to select an action to take: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "#### Greedy Action Selection Method\n",
    "$$ A_t = {argmax}_a Q_t (a) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "#### $\\epsilon$ - Greedy Method\n",
    "\n",
    "* Behave greedily most of the time, \n",
    "* But every once in a while, with small probability $\\epsilon$,\n",
    "  * Select randomly from amongst all the actions ,\n",
    "  * With equal probability independently of the action-value estimates.<br><br>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<center>\n",
    "$\\quad$ A = $argmax_a Q(a) $ with probabiility $ 1-\\epsilon $<br>\n",
    "$\\quad$ $\\quad$ or<br>\n",
    "$\\quad$ A = any random Action with probability $\\epsilon$<br>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Basically, the pseudo code for above algo is:\n",
    "\n",
    "Initialize $Q_t(a) \\quad \\forall a \\in A$,<br>\n",
    "At each time step:<br>\n",
    "$\\quad$Select an action $A_t = a $ by greedy / $\\epsilon$-greedy action selection<br>\n",
    "$\\quad$Calculate new $Q_t$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "#### Minor point: Incremental Implementation\n",
    "<br>\n",
    "\n",
    "$$ Q_n = \\frac{R_1 + R_2+ ... + R_{n-1}}{n-1} $$\n",
    "\n",
    "$$ Q_{n+1} = Q_n + \\frac{1}{n} \\big[ R_n - Q_n\\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## <center>Simple Bandit Algorithm with $\\epsilon$-greedy method</center>\n",
    "\n",
    "\n",
    "Initialize, for a =1 to k:<br>\n",
    "$\\quad$ Q(a) = 0<br>\n",
    "$\\quad$ N(a) = 0<br>\n",
    "Repeat Forever:<br>\n",
    "$\\quad$ A = $argmax_a Q(a) \\quad $ with probabiility $ 1-\\epsilon $<br>\n",
    "$\\quad$ $\\quad$ or<br>\n",
    "$\\quad$ A = any random Action $\\quad$ with probability $\\epsilon$<br>\n",
    "$\\quad$ R = reward(A)<br>\n",
    "$\\quad$ N(A) = N(A) +1<br>\n",
    "$\\quad$ Q(A) = Q(A) + $\\frac{1}{n}\\big[ R_n - Q_n\\big]$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Problem Formulation - The 10-armed testbed</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Problem Formulation - The 10-armed testbed\n",
    "<center><img src=\"img/testbed.jpg\" alt=\"Ten-armed Testbed\" style=\"width: 400px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "We randomly generated k-armed bandit problems with k = 10.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "For each bandit problem,, the action values, $q_∗ (a)$, $a = 1, . . . , 10,$ were selected according to a normal (Gaussian) distribution with mean 0 and variance 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Then,when a learning method applied to that problem selected action $A_t$ at time $t$, the actual reward $R_t$ was selected from a normal distribution with mean $q_∗ (A_t)$ and variance 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* For any learning method, measure performance as it improves with experience over 1000 steps interaction. \n",
    "* This is one run. Repeate this for 2000 independent runs.\n",
    "  * Obtain measures of the learning algorithm’s **average** behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "<center><img src=\"img/greedyvs.jpg\" alt=\"Ten-armed Testbed\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Question\n",
    "\n",
    "When would $epsilon$-greedy method perform significantly better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Update Rule of Simple Bandit Problem\n",
    "\n",
    "In the pseudocode, given by:<br>\n",
    "<center> Q(A) = Q(A) + $\\frac{1}{n}\\big[ R_n - Q_n\\big]$</center><br>\n",
    "\n",
    "This can be written as:<br>\n",
    "<center>NewEstimate = OldEstimate + StepSize * [Target − OldEstimate]</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tracking a Nonstationary Problem\n",
    "\n",
    "Some problems are nonstationary, meaning reward function is not from a constant distribution.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Problem in Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In such cases, weight recent rewards more heavily than long-past ones.<br>\n",
    "This is done by:\n",
    "\n",
    "$$Q_{n+1} = Q_n + \\alpha [R_n − Q_n]$$\n",
    "$$\\alpha \\in (0,1] $$\n",
    "\n",
    "This method is called: exponential, recency-weighted average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Improving exploration in Simple Bandit Problems</center>\n",
    "\n",
    "#### <center>(These methods would be used in different forms in future methods)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "#### Optimistic initial values\n",
    "\n",
    "These methods are biased by their initial estimates.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"img/testbed.jpg\" alt=\"Ten-armed Testbed\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Results\n",
    "\n",
    "\n",
    "<center><img src=\"img/optinit.jpg\" alt=\"Optimistic Initialization\" style=\"width: 800px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Optimistic initial values\n",
    "\n",
    "* For the sample-average methods, the bias disappears once all actions have been selected at least once, <br>\n",
    "* methods with constant $\\alpha$, the bias is permanent, though decreasing over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Initial action values can be used as a simple way of encouraging exploration.\n",
    "\n",
    "* However, this boost the exploration only once, in the start. It **cannot be very useful in a non-stationary process**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Upper-Confidence-Bound Action Selection\n",
    "\n",
    "$$ A_t= argmax_a \\bigg[ Q_t (a) + c \\sqrt{\\frac{log(t)}{N_t (a)}} \\bigg]  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "In upper confidence bound (UCB) action selection, the squareroot term is a measure of the uncertainty or variance in the estimate of $a$’s value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Results\n",
    "\n",
    "\n",
    "<center><img src=\"img/UCB.jpg\" alt=\"UCB\" style=\"width: 800px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Gradient Bandit Algorithms\n",
    "\n",
    "* Numerical preference $H_t (a)$ for each action $a$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The larger the preference, the more often that action is taken,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Only the relative preference of one action over another is important;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$$ Pr\\{ A_t =a\\} = \\frac{e^{H_t (a)}}{\\sum^{k}_{b=1}{e^{H_t (b)}}} = \\pi_t (a) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "** Update Rules **\n",
    "\n",
    "$$ H_{t+1}(A_t) = H_t (A_t) + \\alpha(R_t − \\overline{R_{t}})(1− \\pi_t (A_t)) $$\n",
    "and\n",
    "\n",
    "$$ H_{t+1} (a) = H_t (a) + \\alpha(R_t − \\overline{R_{t}})\\pi_t (a) \\quad \\forall a \\neq A_t $$\n",
    "\n",
    "$\\overline{R_{t}}$ is the average of all the rewards up through and including time t. It is also called the **baseline reward**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Results\n",
    "\n",
    "\n",
    "<center><img src=\"img/grad.jpg\" alt=\"Gradient Bandit\" style=\"width: 800px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Summary</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We have seen how Reinforcement learning is different from other forms of learning.<br>\n",
    "* We applied reinforcement learning to a very simple problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center>Summary</center>\n",
    "\n",
    "#### Comparison between the learnt methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The ε-greedy methods choose randomly a small fraction of the time,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* UCB methods choose deterministically but achieve exploration by subtly favoring at each step the actions that have so far received fewer samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Gradient bandit algorithms estimate not action values, but action preferences, and favor the more preferred actions in a graded, probabilistic manner using a soft-max distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "*  The simple expedient of initializing estimates optimistically causes even greedy methods to explore significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Which is the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"img/com_2.jpg\" alt=\"Gradient Bandit\" style=\"width: 800px;\"/></center>\n",
    "\n",
    "For the given problem, UCB performs the best. \n",
    "\n",
    "**No general statement should be made.**\n",
    "\n",
    "Different methods may perform better at different problems!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "96d1e7b8-8b30-4fe9-afb9-753239a40c22": {
     "id": "96d1e7b8-8b30-4fe9-afb9-753239a40c22",
     "layout": "treemap",
     "prev": null,
     "regions": {}
    }
   },
   "themes": {
    "default": "251b2c57-06e5-47d5-9096-023bf40e7dd6",
    "theme": {
     "251b2c57-06e5-47d5-9096-023bf40e7dd6": {
      "id": "251b2c57-06e5-47d5-9096-023bf40e7dd6",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         155,
         177,
         192
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410"
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 8
       },
       "h2": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "font-family": "Merriweather",
       "font-size": 4
      }
     },
     "5d0c7a62-b48a-4928-bc95-261e0dfc25ba": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "5d0c7a62-b48a-4928-bc95-261e0dfc25ba",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         34,
         34,
         34
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         256,
         256,
         256
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         66,
         175,
         250
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         256,
         256,
         256
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 5.25
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 4
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 3.5
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Source Sans Pro",
        "font-size": 6
       },
       "p": {
        "color": "mainColor",
        "font-family": "Source Sans Pro",
        "font-size": 6
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Source Sans Pro",
       "font-size": 6
      }
     },
     "684b3e95-11c6-4e04-8b94-30ded68d1881": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "684b3e95-11c6-4e04-8b94-30ded68d1881",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197,
         226,
         245
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4
      }
     },
     "7654ff49-ace1-43a1-8a29-9eb4c66669a1": {
      "id": "7654ff49-ace1-43a1-8a29-9eb4c66669a1",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         155,
         177,
         192
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410"
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 8
       },
       "h2": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "font-family": "Merriweather",
       "font-size": 4
      }
     },
     "8117b356-2708-4ed9-aa83-e37004d18cd1": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "8117b356-2708-4ed9-aa83-e37004d18cd1",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         34,
         34,
         34
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         256,
         256,
         256
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         66,
         175,
         250
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         256,
         256,
         256
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 5.25
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 4
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 3.5
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Source Sans Pro",
        "font-size": 6
       },
       "p": {
        "color": "mainColor",
        "font-family": "Source Sans Pro",
        "font-size": 6
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Source Sans Pro",
       "font-size": 6
      }
     },
     "fd0b4f88-e441-427c-84e4-74d006f476bd": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "fd0b4f88-e441-427c-84e4-74d006f476bd",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197,
         226,
         245
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4
      }
     }
    }
   }
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

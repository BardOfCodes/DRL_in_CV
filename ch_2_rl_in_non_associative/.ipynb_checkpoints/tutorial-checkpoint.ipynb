{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Introduction to Multi_arm_bandits\n",
    "import numpy as np\n",
    "import multi_arm_bandits.multi_bandit_env.multi_arm_bandit as mab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Now you have bandits as well as multi-arm bandits.\n",
    "# Lets see some of the bandits\n",
    "bandit_1 = mab.bandit().normal(0,1)\n",
    "\n",
    "# when triggered, bandit_1 outputs a reward sample from a gaussian distribution with mean 0 and std 1.\n",
    "print(bandit_1.get_reward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bandit_2 = mab.bandit().uniform(2,3)\n",
    "# we can find our expected return from this bandit after say a 1000 trials\n",
    "\n",
    "bandit_2_returns = # Write Code here!\n",
    "\n",
    "print(bandit_2_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now a single bandit might not be so interesting, we will deal with multiple bandits or multi-arm-bandits\n",
    "mab_1 = mab.multi_arm_bandit().gaussian(10, 5, 2, 1)\n",
    "# constructs a multi_arm_bandit, with 10 arms, each having a N(r,1) reward distribution, \n",
    "# where 'r' is sampled from a Normal(5,2) distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To get reward now we must supply our arm-selection, lets say arm-4\n",
    "\n",
    "reward = mab_1.get_reward(3)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can see the actual parameters of the arm-4, the mean and the std:\n",
    "print(mab_1.bandit_list[3].params)\n",
    "# we can see if our estimate of this mean value:\n",
    "\n",
    "mab_1_4_returns = # Write code here! \n",
    "\n",
    "print(mab_1_4_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we form the 10-arm test-bed, which is a 10-armed bandit with each arm having a N(r,1) reward distribution, \n",
    "# where 'r' is sampled from a Normal(0,1) distribution.\n",
    "test_bed = mab.multi_arm_bandit().gaussian(10, 0, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualizing the rewards distribution\n",
    "reward_means = [x.params[0] for x in test_bed.bandit_list]\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1,11), reward_means,'ro')\n",
    "plt.xlabel(\"Action\")\n",
    "plt.ylabel(\"Reward distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Each observation is made from 1000 steps on the test bed\n",
    "# Each test is repeated 2000 times to get an estimate of the performance\n",
    "\n",
    "# lets use a naive approach, we try each method once, and select the one with highest outcome as action for the next 990 steps.\n",
    "val_estimate = np.zeros((10))\n",
    "for i in range(10):\n",
    "    val_estimate[i] = test_bed.get_reward(i)\n",
    "\n",
    "# This is going to be my action selection for the next 990 runs\n",
    "print('Selected action',np.argmax(val_estimate))\n",
    "\n",
    "selected_action = np.argmax(val_estimate)\n",
    "# we can keep tab of our performance:\n",
    "avg_reward_list = [np.sum(val_estimate[0:i])/float(i) for i in range(1,11)]\n",
    "avg_reward = avg_reward_list[9]\n",
    "for i in range(10,1000):\n",
    "    cur_reward = test_bed.get_reward(selected_action)\n",
    "    avg_reward +=(cur_reward-avg_reward)/float(i)\n",
    "    avg_reward_list.append(avg_reward)\n",
    "\n",
    "print('Average Reward',avg_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <center>Simple Bandit Algorithm with $\\epsilon$-greedy method</center>\n",
    "\n",
    "\n",
    "Initialize, for a =1 to k:<br>\n",
    "$\\quad$ Q(a) = 0<br>\n",
    "$\\quad$ N(a) = 0<br>\n",
    "Repeat Forever:<br>\n",
    "$\\quad$ A = $argmax_a Q(a) \\quad $ with probabiility $ 1-\\epsilon $<br>\n",
    "$\\quad$ $\\quad$ or<br>\n",
    "$\\quad$ A = any random Action $\\quad$ with probability $\\epsilon$<br>\n",
    "$\\quad$ R = reward(A)<br>\n",
    "$\\quad$ N(A) = N(A) +1<br>\n",
    "$\\quad$ Q(A) = Q(A) + $\\frac{1}{n}\\big[ R_n - Q_n\\big]$<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write the code implementation of the above pseudo-code for 2000 itertions\n",
    "# and print the total accumulated reward\n",
    "epsilon = 0.1\n",
    "actions = 10\n",
    "Q = np.zeros(actions)\n",
    "N = np.zeros(actions)\n",
    "total_reward = 0\n",
    "for i in range(1000):\n",
    "    \n",
    "    # Write Code here!\n",
    "\n",
    "print(total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The action selector is a function, which takes as input \n",
    "# a array Q and an array N to output the action A at that time step.\n",
    "\n",
    "# Given below is an example of a function which will return our epsilon greedy function, given epsilon.\n",
    "\n",
    "def create_epsilon_greedy_selector(epsilon):\n",
    "    \n",
    "    def action_selector(Q,N):\n",
    "        p = np.random.uniform(0,1)\n",
    "        if p>epsilon:\n",
    "            cur_action = np.argmax(Q)\n",
    "        else:\n",
    "            cur_action = np.random.choice(np.arange(len(Q)),None)\n",
    "        return cur_action\n",
    "    \n",
    "    return action_selector\n",
    "\n",
    "# For example\n",
    "epsilon_selector = create_epsilon_greedy_selector(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can convert this run into a simulation/function\n",
    "# Takes as input, the test_bed, max_iteration,and the action_selector\n",
    "# and returns a list with the reward for each iteration,\n",
    "# and a list containing 1 if the optimal action was taken, 0 otherwise\n",
    "\n",
    "def bandit_simulation(mab,max_iter,action_selector):\n",
    "    rewards = np.zeros((max_iter))\n",
    "    obs_optimal = np.zeros((max_iter))\n",
    "    optimal_action = np.argmax(np.array([x.params[0] for x in mab.bandit_list]))\n",
    "    Q = np.zeros(len(mab.bandit_list))\n",
    "    N = np.zeros(len(mab.bandit_list))\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        \n",
    "        # Write Code Here!\n",
    "    \n",
    "    return rewards, obs_optimal\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we want to find the average performance for different values of epsilon\n",
    "# Run the Above algorithm for 2,000 times, \n",
    "# In each time keep a tab on the average acumalated reward over the thousand steps.\n",
    "\n",
    "## Make varients of epsilon-greedy action selector from here\n",
    "epsilon_selector = create_epsilon_greedy_selector(0.1)\n",
    "\n",
    "max_iter = 1000\n",
    "global_avg_rewards = np.zeros((max_iter))\n",
    "global_obs_optimal = np.zeros((max_iter))\n",
    "for i in range(2000):\n",
    "    # Create a new test_bed\n",
    "    test_bed = mab.multi_arm_bandit().gaussian(10, 0, 1, 1)\n",
    "    cur_rewards, cur_obs_optimal = bandit_simulation(test_bed,max_iter,epsilon_selector)\n",
    "    global_avg_rewards +=(cur_rewards-global_avg_rewards)/float(i+1)\n",
    "    global_obs_optimal +=(cur_obs_optimal-global_obs_optimal)/float(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally!\n",
    "# Plot the performance!\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1,max_iter+1),global_obs_optimal)\n",
    "plt.ylabel(\"fraction of optimal actions\")\n",
    "plt.xlabel(\"Time-steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1,max_iter+1),global_avg_rewards)\n",
    "plt.xlabel(\"Time-steps\")\n",
    "plt.ylabel(\"Average_rewards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to encorage exploration\n",
    "\n",
    "Now we will implement:\n",
    "* Optimistic Initial Values\n",
    "* Upper-Confidence Bound Action selection\n",
    "* Gradient Bandit Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimistic initial value\n",
    "# Just start off with an optimistic initial value\n",
    "# Modify the bandit_simulation function to have optimistic inital values\n",
    "\n",
    "def optimistic_bandit_simulation(mab,max_iter,action_selector,Q_val=None):\n",
    "    rewards = np.zeros((max_iter))\n",
    "    obs_optimal = np.zeros((max_iter))\n",
    "    optimal_action = np.argmax(np.array([x.params[0] for x in mab.bandit_list]))\n",
    "    \n",
    "    # Write Code for Q here, which is Q_val if given or max(mean(reward)+1.5\n",
    "\n",
    "    N = np.zeros(len(mab.bandit_list))\n",
    "    # print(best_reward,Q_val)\n",
    "    for i in range(max_iter):\n",
    "        cur_action = action_selector(Q,N)\n",
    "        cur_reward = test_bed.get_reward(cur_action)\n",
    "        N[cur_action] +=1\n",
    "        Q[cur_action] += (cur_reward-Q[cur_action])/N[cur_action]\n",
    "        \n",
    "        # Or try Step size instead of N[cur_action] for Q[cur_action]\n",
    "        \n",
    "        rewards[i] = cur_reward#-avg_rewards[i-1])/float(i+1)\n",
    "        if cur_action == optimal_action:\n",
    "            obs_optimal[i] =1\n",
    "        else:\n",
    "            obs_optimal[i] =0\n",
    "    return rewards, obs_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets see how it performs\n",
    "\n",
    "epsilon_selector = create_epsilon_greedy_selector(0.1)\n",
    "\n",
    "max_iter = 1000\n",
    "global_avg_rewards_optimistic = np.zeros((max_iter))\n",
    "global_obs_optimal_optimistic = np.zeros((max_iter))\n",
    "for i in range(2000):\n",
    "    # Create a new test_bed\n",
    "    test_bed = mab.multi_arm_bandit().gaussian(10, 0, 1, 1)\n",
    "    cur_rewards, cur_obs_optimal = optimistic_bandit_simulation(test_bed,max_iter,epsilon_selector,5)\n",
    "    global_avg_rewards_optimistic +=(cur_rewards-global_avg_rewards_optimistic)/float(i+1)\n",
    "    global_obs_optimal_optimistic +=(cur_obs_optimal-global_obs_optimal_optimistic)/float(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally!\n",
    "# Plot the performance!\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1,max_iter+1),global_obs_optimal,'blue',label=\"Simple\")\n",
    "plt.plot(np.arange(1,max_iter+1),global_obs_optimal_optimistic,'red',label='Optimistic')\n",
    "plt.ylabel(\"fraction of optimal actions\")\n",
    "plt.xlabel(\"Time-steps\")\n",
    "plt.legend(loc='best')\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1,max_iter+1),global_avg_rewards,'blue',label=\"Simple\")\n",
    "plt.plot(np.arange(1,max_iter+1),global_avg_rewards_optimistic,'red',label=\"Optimistic\")\n",
    "plt.xlabel(\"Time-steps\")\n",
    "plt.ylabel(\"Average_rewards\")\n",
    "plt.ylabel(\"Average_rewards\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Upper-Confidence-Bound Action Selection\n",
    "\n",
    "$$ A_t= argmax_a \\bigg[ Q_t (a) + c \\sqrt{\\frac{log(t)}{N_t (a)}} \\bigg]  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UCB\n",
    "# write the code for the UCB-function, \n",
    "# Its an action selector function like before\n",
    "\n",
    "def create_UCB_selector(c):\n",
    "    \n",
    "    def action_selector(Q,N):\n",
    "        \n",
    "        # Write Code here\n",
    "        \n",
    "        return cur_action\n",
    "    \n",
    "    return action_selector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## Make varients of epsilon-greedy action selector from here\n",
    "UCB_selector = create_UCB_selector(2)\n",
    "\n",
    "max_iter = 1000\n",
    "global_avg_rewards_UCB = np.zeros((max_iter))\n",
    "global_obs_optimal_UCB = np.zeros((max_iter))\n",
    "for i in range(2000):\n",
    "    # Create a new test_bed\n",
    "    test_bed = mab.multi_arm_bandit().gaussian(10, 0, 1, 1)\n",
    "    cur_rewards, cur_obs_optimal = bandit_simulation(test_bed,max_iter,UCB_selector)\n",
    "    global_avg_rewards_UCB +=(cur_rewards-global_avg_rewards_UCB)/float(i+1)\n",
    "    global_obs_optimal_UCB +=(cur_obs_optimal-global_obs_optimal_UCB)/float(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gradient Bandit\n",
    "# Plot the performance!\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1,max_iter+1),global_obs_optimal,'blue',label=\"Simple\")\n",
    "plt.plot(np.arange(1,max_iter+1),global_obs_optimal_UCB,'red',label='UCB')\n",
    "plt.ylabel(\"fraction of optimal actions\")\n",
    "plt.xlabel(\"Time-steps\")\n",
    "plt.legend(loc='best')\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1,max_iter+1),global_avg_rewards,'blue',label=\"Simple\")\n",
    "plt.plot(np.arange(1,max_iter+1),global_avg_rewards_UCB,'red',label=\"UCB\")\n",
    "plt.xlabel(\"Time-steps\")\n",
    "plt.ylabel(\"Average_rewards\")\n",
    "plt.ylabel(\"Average_rewards\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Bandit\n",
    "\n",
    "** Update Rules **\n",
    "\n",
    "$$ H_{t+1}(A_t) = H_t (A_t) + \\alpha(R_t − \\overline{R_{t}})(1− \\pi_t (A_t)) $$\n",
    "and\n",
    "\n",
    "$$ H_{t+1} (a) = H_t (a) + \\alpha(R_t − \\overline{R_{t}})\\pi_t (a) \\quad \\forall a \\neq A_t $$\n",
    "\n",
    "$\\overline{R_{t}}$ is the average of all the rewards up through and including time t. It is also called the **baseline reward**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient Bandit\n",
    "# We now have an update rule for H, rather than Q.\n",
    "# Additionally the selection of action is done on the basis of its probability\n",
    "# However, as the update rules are new,\n",
    "# We make a new bandit similation function\n",
    "\n",
    "def gradient_bandit_simulation(mab,max_iter,alpha,use_b=True,baseline=0):\n",
    "    rewards = np.zeros((max_iter))\n",
    "    obs_optimal = np.zeros((max_iter))\n",
    "    optimal_action = np.argmax(np.array([x.params[0] for x in mab.bandit_list]))\n",
    "    H = np.zeros(len(mab.bandit_list))\n",
    "    N = np.zeros(len(mab.bandit_list))\n",
    "    # print(best_reward,Q_val)\n",
    "    for i in range(max_iter):\n",
    "        \n",
    "        # Write Code here !\n",
    "        \n",
    "    return rewards, obs_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 1000\n",
    "global_avg_rewards_GB = np.zeros((max_iter))\n",
    "global_obs_optimal_GB = np.zeros((max_iter))\n",
    "for i in range(2000):\n",
    "    # Create a new test_bed\n",
    "    test_bed = mab.multi_arm_bandit().gaussian(10, 0, 1, 1)\n",
    "    cur_rewards, cur_obs_optimal = gradient_bandit_simulation(test_bed,max_iter,0.4)\n",
    "    global_avg_rewards_GB +=(cur_rewards-global_avg_rewards_GB)/float(i+1)\n",
    "    global_obs_optimal_GB +=(cur_obs_optimal-global_obs_optimal_GB)/float(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gradient Bandit\n",
    "# Plot the performance!\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1,max_iter+1),global_obs_optimal,'blue',label=\"Simple\")\n",
    "plt.plot(np.arange(1,max_iter+1),global_obs_optimal_GB,'red',label='Gradient Bandit')\n",
    "plt.ylabel(\"fraction of optimal actions\")\n",
    "plt.xlabel(\"Time-steps\")\n",
    "plt.legend(loc='best')\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1,max_iter+1),global_avg_rewards,'blue',label=\"Simple\")\n",
    "plt.plot(np.arange(1,max_iter+1),global_avg_rewards_GB,'red',label=\"Gradient Bandit\")\n",
    "plt.xlabel(\"Time-steps\")\n",
    "plt.ylabel(\"Average_rewards\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Additionally, you can try and compare Gradient bandit with and without Baseline. \n",
    "# We hope the user is comfortable enough with this interface now\n",
    "# To construct such an experiment by themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end!\n",
    "\n",
    "Hopefully we got a taste of reinforcement learning and learnt some very useful exploration techniques!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
